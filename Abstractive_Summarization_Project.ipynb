{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Importing stock libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration\n",
    "\n",
    "# Use 0 for t5, 1 for bart and 2 for ProphetNet\n",
    "choosen_model = 1\n",
    "data_augmentation = True\n",
    "conclusion_included = True\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda:1' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Define a function to extract abstract, body and conclusion of the articles cointained in the PDF files </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_articles(pdf_path):\n",
    "    abstracts = []\n",
    "    bodies = []\n",
    "    conclusions = []\n",
    "    \n",
    "    for file in os.listdir(pdf_path):\n",
    "        \n",
    "        path = os.path.join(pdf_path, file)\n",
    "        print(path)\n",
    "\n",
    "        # Estract the text from the PDF file\n",
    "        pdf_text = extract_text(path)\n",
    "\n",
    "        # Define a regex to detect the abstract\n",
    "        pattern = re.compile(r'Abstract\\. (.*?)(?=\\n\\n(?:Keywords|1 Introduction|Introduction))', re.DOTALL| re.IGNORECASE)\n",
    "        matches = pattern.finditer(pdf_text)\n",
    "        matches_list = list(matches)\n",
    "        print(\"Numero di articoli trovati:\", len(matches_list))\n",
    "        \n",
    "        for i,match in enumerate(matches_list):\n",
    "            \n",
    "            if i<len(matches_list)-1:\n",
    "\n",
    "                next_article_start = matches_list[i+1].start()\n",
    "            else:\n",
    "\n",
    "                next_article_start = -1\n",
    "\n",
    "            abstract_start = match.start()+ 9 \n",
    "            abstract_end = match.end()\n",
    "\n",
    "            body_start = pdf_text.find('Introduction', abstract_end) + 12\n",
    "\n",
    "            # Define a regex to detect the start of the conclusions\n",
    "            pattern = re.compile(r'\\n\\d{1,2}\\sConclusion')\n",
    "            match = pattern.search(pdf_text[body_start: next_article_start]) \n",
    "            \n",
    "            if match:\n",
    "                body_end = body_start + match.start()\n",
    "            else:\n",
    "                body_end = -1            \n",
    "\n",
    "            # If not explicit conluclion are found there are some other possibilities\n",
    "            if body_end == -1:\n",
    "                endings = ['Discussion and Conclusion','Summary','Current Challenges','Result','Evaluation', 'Conclusion']\n",
    "                for end in endings: \n",
    "                    pattern = re.compile(r'\\b\\d+(\\.\\d+)?\\s' + re.escape(end))\n",
    "                    alt_match = pattern.search(pdf_text[body_start: next_article_start])\n",
    "                    \n",
    "                    if alt_match:\n",
    "                        body_end = body_start + alt_match.start()\n",
    "                        print(f\"Article {i} instead of Conclusion has {end} as ending\")\n",
    "                        break\n",
    "                if body_end == -1:\n",
    "                    print(f\"No conclusion found for article {i}\")\n",
    "                    \n",
    "            else:\n",
    "                next_line_index = pdf_text.find('\\n', body_end+1)\n",
    "                if next_line_index != -1:\n",
    "                    body_end = next_line_index\n",
    "\n",
    "            abstract = pdf_text[abstract_start:abstract_end].strip()\n",
    "            body = pdf_text[body_start:body_end - 12].strip()\n",
    "\n",
    "            conclusion_start =pdf_text.find('\\n', body_end + 1) \n",
    "            conclusion_end = pdf_text.find('References', body_end) \n",
    "            conclusion = pdf_text[conclusion_start:conclusion_end].strip()\n",
    "\n",
    "            conclusions.append(conclusion)\n",
    "            abstracts.append(abstract)\n",
    "            bodies.append(body)\n",
    "\n",
    "      \n",
    "    return abstracts, bodies, conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract the informations from the files</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Documents/978-3-031-41676-7-Part-I.pdf\n",
      "Numero di articoli trovati: 31\n",
      "./Documents/978-3-031-41679-8-Part-II.pdf\n",
      "Numero di articoli trovati: 36\n",
      "Article 4 instead of Conclusion has Conclusion as ending\n",
      "./Documents/978-3-031-41685-9-Part-IV-Handwriting.pdf\n",
      "Numero di articoli trovati: 28\n",
      "./Documents/978-3-031-41682-8-Part-III-Document-NLP.pdf\n",
      "Numero di articoli trovati: 30\n",
      "./Documents/978-3-031-41734-4-Part-V.pdf\n",
      "Numero di articoli trovati: 33\n",
      "Article 11 instead of Conclusion has Summary as ending\n",
      "Article 20 instead of Conclusion has Discussion and Conclusion as ending\n",
      "./Documents/978-3-031-41731-3-Scene-Text-Part-VI.pdf\n",
      "Numero di articoli trovati: 10\n",
      "./Documents/978-3-031-41498-5-Workshops-I.pdf\n",
      "Numero di articoli trovati: 22\n",
      "Article 6 instead of Conclusion has Current Challenges as ending\n",
      "Article 13 instead of Conclusion has Evaluation as ending\n",
      "Article 19 instead of Conclusion has Discussion and Conclusion as ending\n",
      "./Documents/978-3-031-41501-2-Workshops-II.pdf\n",
      "Numero di articoli trovati: 21\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = './Documents' \n",
    "abstract_texts, body_texts, conclusion_texts = extract_articles(pdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define a function to clear the gatherated text </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # To discard some common string in the article related to the conference\n",
    "    text = re.sub(r'c\\(cid\\:2\\) The Author\\(s\\), under exclusive license to Springer Nature Switzerland AG 2023','',text)\n",
    "    text = re.sub(r'G\\. A\\. Fink et al\\. \\(Eds\\.\\)\\: ICDAR 2023, LNCS \\d{5}, pp\\. \\d{1,4}\\–\\d{1,4}, 2023\\.','',text)\n",
    "    text = re.sub(r'This work was supported by Grant PID2020\\-116813RB\\-I00 funded by MCIN\\/AEI\\/','',text)\n",
    "    text = re.sub(r'10\\.13039\\/501100011033\\, by Grant ACIF\\/2021\\/436 funded by Generalitat Valen-','',text)\n",
    "    text = re.sub(r'ciana and by Grant PID2021\\-124719OB\\-I00 funded by MCIN\\/AEI\\/10\\.13039\\/5011','',text)\n",
    "    text = re.sub(r'00011033 and by ERDF\\, EU A way of making Europe\\.','',text)\n",
    "    text = re.sub(r'A\\. Banerjee and S\\. Biswas\\—These authors contributed equally to this work\\.','',text)\n",
    "    text = re.sub(r'Md\\. I\\. H\\. Shihab\\, Md\\. R\\. Hasan\\, M\\. R\\. Emon\\, A\\. I\\. Humayun and A\\. Sushmit\\—Equal\\ncontribution\\.','',text)\n",
    "    text = re.sub(r'BaDLAD\\: A Large Multi\\-Domain Bengali Document Layout','',text)\n",
    "    text = re.sub(r'\\nProject website\\:','',text)\n",
    "    text = re.sub(r'L\\. He\\—Work done while a research intern at Microsoft Cloud and AI\\.','',text)\n",
    "    text = re.sub(r'A\\. Scius\\-Bertrand et al\\.','',text)\n",
    "    text = re.sub(r'Z. Yang\\, X\\. Song and S\\. Song\\—Equal Contribution\\.','',text)\n",
    "    text = re.sub(r'\\n\\n.*?\\n\\n\\d+\\n', '', text)\n",
    "    \n",
    "    text = re.sub(r'c\\(cid\\:2\\)', '', text)\n",
    "    text = re.sub(r'\\(cid\\:2\\)', '', text)\n",
    "\n",
    "    # To discard the page number\n",
    "    text = re.sub(r'\\d{1,4}\\s\\n','',text) \n",
    "    text = re.sub(r'\\n\\d+\\.\\d+\\n','\\n',text) \n",
    "    \n",
    "    # To discard the figure reference\n",
    "    text = re.sub(r'\\n\\nFig\\.\\s\\d+\\..*?\\n\\n', '\\n', text, flags=re.DOTALL)\n",
    "\n",
    "    # To discard citations\n",
    "    text = re.sub(r'\\s*\\[\\d+(,\\s*\\d+)*\\]','',text)\n",
    "    text = re.sub(r'\\s*\\[\\d{1,2}\\–\\d{1,2}\\]','',text)\n",
    "    text = re.sub(r'\\s*\\[\\d{1,2},\\d{1,2}\\–\\d{1,2}\\]','',text)\n",
    "    text = re.sub(r'\\n\\x0c[^\\n]*\\n','',text)\n",
    "    text = re.sub(r'\\n[A-Za-z]\\.\\s\\w+\\s+et al\\.','',text)\n",
    "    text = re.sub(r'\\n\\d{1,2}\\s(?=\\w).*', '\\n', text)\n",
    "\n",
    "    text = re.sub(r'\\n\\n','\\n',text)\n",
    "    text = re.sub(r'\\n\\d{1,2}\\n','\\n',text)\n",
    "\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # To discard URLs\n",
    "    text = re.sub(r'https?://[^\\s]+', '', text)  # Rimuove URL\n",
    "\n",
    "    # To merge a sentence that was splitted in two seperated rows\n",
    "    text=re.sub(r'(\\w+)\\s*-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    text = re.sub(r'\\n',' ',text)\n",
    "\n",
    "    # To discard extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'^\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Clean the gathered texts and create a DataFrame</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "for abstract, body, conclusion in zip(abstract_texts,body_texts, conclusion_texts):\n",
    "\n",
    "  article_abstract = \"\".join(clean_text(abstract))\n",
    "  article_body = \"\".join(clean_text(body))\n",
    "  article_conclusion = \"\".join(clean_text(conclusion))\n",
    "  \n",
    "  cleaned_data.append([article_abstract, article_body, article_conclusion])\n",
    "\n",
    "df = pd.DataFrame(cleaned_data, columns=['article_abstract','article_body' ,'article_conclusion'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Store the cleaned texts, in order to avoid repeating the process every time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load the data and create a DataFrame</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_abstract</th>\n",
       "      <th>article_body</th>\n",
       "      <th>article_conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rey-osterrieth complex figure test (rocft)...</td>\n",
       "      <td>the rey-osterrieth complex figure test (rocft)...</td>\n",
       "      <td>in this work, we developed and validated an ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accurately extracting structured data from str...</td>\n",
       "      <td>as typical rich-format business documents, ﬁna...</td>\n",
       "      <td>in this paper, we proposed a new method for st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state-of-the-art oﬄine optical character recog...</td>\n",
       "      <td>semi-structured documents are widely used in m...</td>\n",
       "      <td>in this work, we (i) developed the ﬁrst datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>document binarization plays a key role in info...</td>\n",
       "      <td>document binarization refers to the process of...</td>\n",
       "      <td>in this paper, we have proposed a new binariza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we present a novel sub-stroke level transforme...</td>\n",
       "      <td>in today’s highly virtual and automated world,...</td>\n",
       "      <td>in this paper, we presented a novel sub-stroke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    article_abstract  \\\n",
       "0  the rey-osterrieth complex figure test (rocft)...   \n",
       "1  accurately extracting structured data from str...   \n",
       "2  state-of-the-art oﬄine optical character recog...   \n",
       "3  document binarization plays a key role in info...   \n",
       "4  we present a novel sub-stroke level transforme...   \n",
       "\n",
       "                                        article_body  \\\n",
       "0  the rey-osterrieth complex figure test (rocft)...   \n",
       "1  as typical rich-format business documents, ﬁna...   \n",
       "2  semi-structured documents are widely used in m...   \n",
       "3  document binarization refers to the process of...   \n",
       "4  in today’s highly virtual and automated world,...   \n",
       "\n",
       "                                  article_conclusion  \n",
       "0  in this work, we developed and validated an ap...  \n",
       "1  in this paper, we proposed a new method for st...  \n",
       "2  in this work, we (i) developed the ﬁrst datase...  \n",
       "3  in this paper, we have proposed a new binariza...  \n",
       "4  in this paper, we presented a novel sub-stroke...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-processed data\n",
    "with open('cleaned_data.pkl', 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)\n",
    "\n",
    "# Convert loaded data into DataFrame\n",
    "df = pd.DataFrame(cleaned_data, columns=['article_abstract','article_body', 'article_conclusion'])    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abstract lenght: 274\n"
     ]
    }
   ],
   "source": [
    "# Get the lenght of each abstracy\n",
    "abstract_lengths = df['article_abstract'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Look for the max\n",
    "max_abstract_length = abstract_lengths.max()\n",
    "print(\"Max abstract lenght:\", max_abstract_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Split the dataset and create train, validation and test sets </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: (211, 3)\n",
      "Train dataset size: (169, 3)\n",
      "Validation dataset size: (21, 3)\n",
      "Test dataset size: (21, 3)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_test_split = 0.9 # 90% of the data is used for training and and 10% for testing\n",
    "train_val_split = 0.89\n",
    "\n",
    "# Set a seed for the random split to let the experiment replicable\n",
    "seed = 10 # or random.randint(0, 100)\n",
    "\n",
    "# Split the dataset in train/val and test sets\n",
    "train_val_dataset = df.sample(frac = train_test_split, random_state = seed)\n",
    "test_dataset = df.drop(train_val_dataset.index)\n",
    "\n",
    "# Split train and val set\n",
    "train_dataset = train_val_dataset.sample(frac = train_val_split, random_state = seed)\n",
    "val_dataset = train_val_dataset.drop(train_dataset.index)\n",
    "\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "val_dataset = val_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"Full dataset size: {}\".format(df.shape))\n",
    "print(\"Train dataset size: {}\".format(train_dataset.shape))\n",
    "print(\"Validation dataset size: {}\".format(val_dataset.shape))\n",
    "print(\"Test dataset size: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Implement some Data Augmentation on the texts </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data01/dm23mattri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data01/dm23mattri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data01/dm23mattri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /data01/dm23mattri/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /data01/dm23mattri/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet, stopwords, words\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Inizializza NLTK e spaCy\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "word_list = set(words.words())\n",
    "\n",
    "# Load Word2Vec\n",
    "W2V_path = '/data01/dm23mattri/GoogleNews-vectors-negative300.bin'\n",
    "W2V = KeyedVectors.load_word2vec_format(W2V_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shuffle sentences in the text\n",
    "def permute_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text, language = \"english\")\n",
    "    original_sentences = sentences.copy()\n",
    "    while True:\n",
    "        random.shuffle(sentences)\n",
    "        if sentences != original_sentences:\n",
    "            break\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def get_synonyms_word2vec(word):\n",
    "    synonyms = set()\n",
    "    similar_words = W2V.most_similar(word)\n",
    "    for syn in similar_words:\n",
    "            synonym = syn[0]\n",
    "            synonym = synonym.replace('_', ' ').lower()\n",
    "            if synonym != word:  \n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def get_synonyms_wordnet(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            if synonym != word:  \n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def augment_text_synonims(text, prob = 0.8):\n",
    "    words = nltk.word_tokenize(text, language = \"english\")\n",
    "    augmented_text = []\n",
    "    for word in words:\n",
    "\n",
    "        if word.lower() not in stop_words and word not in string.punctuation:\n",
    "            \n",
    "            if random.random() < prob:\n",
    "                synonyms = get_synonyms_wordnet(word)\n",
    "                #if word in W2V.key_to_index:\n",
    "                #    synonyms = get_synonyms_word2vec(word)\n",
    "                if synonyms:\n",
    "                    word = random.choice(synonyms)\n",
    "\n",
    "        augmented_text.append(word)\n",
    "    return ' '.join(augmented_text)\n",
    "\n",
    "def random_insertion_deletion(text, prob = 0.1):\n",
    "    words = nltk.word_tokenize(text, language = \"english\")\n",
    "    augmented_text = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words and word not in string.punctuation:\n",
    "            if random.random() < prob:\n",
    "                augmented_text.append(random.choice(list(word_list)))\n",
    "            if random.random() > prob:\n",
    "                augmented_text.append(word)\n",
    "        else:\n",
    "            augmented_text.append(word)\n",
    "\n",
    "    return ' '.join(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [04:45<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of the new training set : 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_abstract</th>\n",
       "      <th>article_body</th>\n",
       "      <th>article_conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as text generative models can give increasingl...</td>\n",
       "      <td>with the growing usage of tablets and styluses...</td>\n",
       "      <td>we have presented three improvements to encode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-vqa refers to the set of problems that re...</td>\n",
       "      <td>text-vqa plays an integral role in the automat...</td>\n",
       "      <td>the paper presents a new approach to scene tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>constructing a highly accurate handwritten ocr...</td>\n",
       "      <td>in recent years, researchers in handwritten op...</td>\n",
       "      <td>in this paper, we investigate gc-ddpm to gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>employing a dictionary can eﬃciently rectify t...</td>\n",
       "      <td>deep learning-based scene text recognition has...</td>\n",
       "      <td>in this paper, we propose a new dictionary-gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multi-document summarization (mds) refers to t...</td>\n",
       "      <td>in this era of big data, the fast increase of ...</td>\n",
       "      <td>in this paper, we proposed a new unsupervised ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    article_abstract  \\\n",
       "0  as text generative models can give increasingl...   \n",
       "1  text-vqa refers to the set of problems that re...   \n",
       "2  constructing a highly accurate handwritten ocr...   \n",
       "3  employing a dictionary can eﬃciently rectify t...   \n",
       "4  multi-document summarization (mds) refers to t...   \n",
       "\n",
       "                                        article_body  \\\n",
       "0  with the growing usage of tablets and styluses...   \n",
       "1  text-vqa plays an integral role in the automat...   \n",
       "2  in recent years, researchers in handwritten op...   \n",
       "3  deep learning-based scene text recognition has...   \n",
       "4  in this era of big data, the fast increase of ...   \n",
       "\n",
       "                                  article_conclusion  \n",
       "0  we have presented three improvements to encode...  \n",
       "1  the paper presents a new approach to scene tex...  \n",
       "2  in this paper, we investigate gc-ddpm to gener...  \n",
       "3  in this paper, we propose a new dictionary-gui...  \n",
       "4  in this paper, we proposed a new unsupervised ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "augmented_data = []\n",
    "\n",
    "for index, row in tqdm(train_dataset.iterrows(), total=train_dataset.shape[0]):\n",
    "    \n",
    "    if data_augmentation:\n",
    "\n",
    "        # Extract a row from the dataframe\n",
    "        original_abstract = row['article_abstract']\n",
    "        original_body = row['article_body']\n",
    "        original_conclusion = row['article_conclusion']\n",
    "\n",
    "        # Apply data augmentation\n",
    "        augmented_body_synonym = augment_text_synonims(original_body) \n",
    "        augmented_body_permuted = permute_sentences(original_body)\n",
    "        augmented_body_insert_delete = random_insertion_deletion(original_body)\n",
    "\n",
    "        if conclusion_included:\n",
    "            augmented_conclusion_synonym = augment_text_synonims(original_conclusion)\n",
    "            augmented_conclusion_permuted = permute_sentences(original_conclusion)\n",
    "            augmented_conclusion_insert_delete = random_insertion_deletion(original_conclusion)\n",
    "        else:\n",
    "            augmented_conclusion_synonym = original_conclusion\n",
    "            augmented_conclusion_permuted = original_conclusion\n",
    "            augmented_conclusion_insert_delete = original_conclusion\n",
    "\n",
    "        # Append the augmented data as a dict\n",
    "        augmented_data.append({'article_abstract': original_abstract,'article_body': augmented_body_synonym, 'article_conclusion': augmented_conclusion_synonym})\n",
    "        augmented_data.append({'article_abstract': original_abstract,'article_body': augmented_body_permuted, 'article_conclusion': augmented_conclusion_permuted})\n",
    "        augmented_data.append({'article_abstract': original_abstract,'article_body': augmented_body_insert_delete, 'article_conclusion': augmented_conclusion_insert_delete})\n",
    "\n",
    "# Convert the list to a df\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Combine the two dfs\n",
    "df_combined = pd.concat([train_dataset, df_augmented], ignore_index=True)\n",
    "\n",
    "print(\"Lenght of the new training set :\", len(df_combined))\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Choose the tokenizer </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "if choosen_model == 0:\n",
    "   tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", legacy = False)\n",
    "   print(\"T5 tokenizer loaded\")\n",
    "elif choosen_model== 1:\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "    print(\"BART tokenizer loaded\")\n",
    "elif choosen_model == 2:\n",
    "    tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')\n",
    "    print(\"ProphetNet tokenizer loaded\")\n",
    "else:\n",
    "    raise TypeError(\"Error: wrong number has been use. Use 1,2 or 3 to select the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define the training parameters</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_MAX_LEN = 512 # Length of the texts passed as input to the model\n",
    "SUMMARY_MAX_LEN = 275 # Lenghts of the summary generated by the model\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Prepare the dataset to be used for training </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_max_len, summary_max_len, conclusions = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_max_len = source_max_len\n",
    "        self.summary_max_len = summary_max_len\n",
    "        self.article_abstract = self.data.article_abstract\n",
    "        self.article_body = self.data.article_body\n",
    "        self.article_conclusion = self.data.article_conclusion\n",
    "        self.included_conclusions = conclusions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        article_abstract = str(self.article_abstract[index])\n",
    "        article_abstract = ' '.join(article_abstract.split())\n",
    "\n",
    "        article_body = str(self.article_body[index])\n",
    "        article_body = ' '.join(article_body.split())\n",
    "\n",
    "        if choosen_model == 0:\n",
    "          article_body = 'summarize: ' + article_body\n",
    "\n",
    "        article_conclusion = self.article_conclusion[index]\n",
    "        article_conclusion = ' '.join(article_conclusion.split())\n",
    "\n",
    "        if self.included_conclusions:\n",
    "\n",
    "            # Tokenize body and coclusions\n",
    "            tokenized_body = self.tokenizer.encode_plus(article_body, max_length = self.source_max_len, truncation = True, padding = 'max_length', return_tensors = 'pt')\n",
    "            tokenized_conclusion = self.tokenizer.encode_plus(article_conclusion, max_length = self.source_max_len, truncation = True, padding = 'max_length', return_tensors = 'pt')\n",
    "\n",
    "            # Combine the first 256 token of the two sets and the respective attention masks\n",
    "            combined_input_ids = torch.cat([tokenized_body['input_ids'][:, :256], tokenized_conclusion['input_ids'][:, :256]], dim = 1)\n",
    "            combined_attention_mask = torch.cat([tokenized_body['attention_mask'][:, :256], tokenized_conclusion['attention_mask'][:, :256]], dim = 1)\n",
    "\n",
    "            # Produce the new source and target items\n",
    "            source = {\"input_ids\": combined_input_ids, \"attention_mask\": combined_attention_mask}\n",
    "            target = self.tokenizer.batch_encode_plus([article_abstract], max_length = self.summary_max_len, truncation = True, padding = 'max_length', return_tensors = 'pt')\n",
    "        else:\n",
    "            source = self.tokenizer.batch_encode_plus([article_body], max_length = self.source_max_len, padding = 'max_length', truncation = True, return_tensors = 'pt') \n",
    "            target = self.tokenizer.batch_encode_plus([article_abstract], max_length = self.summary_max_len, padding = 'max_length', truncation = True, return_tensors = 'pt') \n",
    "    \n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_mask': target_mask.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(df_combined, tokenizer, TEXT_MAX_LEN, SUMMARY_MAX_LEN, conclusions = True)\n",
    "validation_set = CustomDataset(val_dataset, tokenizer, TEXT_MAX_LEN, SUMMARY_MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, TEXT_MAX_LEN, SUMMARY_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0,\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 0,\n",
    "             }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Check the configuration of the model to better understand its charateristics </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config, ProphetNetConfig ,BartConfig\n",
    "\n",
    "if choosen_model == 0:\n",
    "    config = T5Config.from_pretrained(\"t5-base\")\n",
    "elif choosen_model== 1:\n",
    "    config = BartConfig.from_pretrained('facebook/bart-base')\n",
    "elif choosen_model == 2:\n",
    "    config =  ProphetNetConfig.from_pretrained('microsoft/prophetnet-large-uncased')\n",
    "    config.use_cache = False\n",
    "else:\n",
    "    raise TypeError(\"Error: wrong number has been use. Use 1,2 or 3 to select the model\")\n",
    "\n",
    "#print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Eventually edit the model's configuration </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually introduce some level of dropout\n",
    "dropout_prob = 0.2\n",
    "\n",
    "if choosen_model == 0:\n",
    "    config.classifier_dropout = dropout_prob\n",
    "    config.dropout_rate = dropout_prob\n",
    "    pass\n",
    "elif choosen_model== 1:\n",
    "    config.activation_dropout = dropout_prob\n",
    "    config.attention_dropout = dropout_prob\n",
    "    #config.encoder_layerdrop = dropout_prob\n",
    "    #config.decoder_layerdrop = dropout_prob\n",
    "    config.classif_dropout = dropout_prob\n",
    "    #config.classifier_dropout = dropout_prob\n",
    "    config.dropout = dropout_prob\n",
    "elif choosen_model == 2:\n",
    "    config.activation_dropout = dropout_prob\n",
    "    config.attention_dropout = dropout_prob\n",
    "    config.dropout = dropout_prob\n",
    "else:\n",
    "    raise TypeError(\"Error: wrong number has been use. Use 1,2 or 3 to select the model\")\n",
    "    \n",
    "#print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Define function to train and evaluate the model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for  data in val_loader:\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels)\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_val_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    return avg_epoch_val_loss\n",
    "\n",
    "def train(model,optimizer, epoch, training_loader, validation_loader):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for data in training_loader :\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels)\n",
    "        loss = outputs[0]\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # After an epoch evaluate the validation loss\n",
    "    val_loss = evaluate(model, validation_loader)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    avg_epoch_train_loss = train_loss/len(training_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Training loss: {avg_epoch_train_loss}, Validation loss: {val_loss}, Time: {epoch_time}, LR: \", optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    return train_loss/ len(training_loader), val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Use Optuna to find the best hyperparameters to train the model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-24 18:40:56,025] A new study created in memory with name: no-name-7c540e7c-f49f-4e78-9cf6-5e81e144bd56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2b37bf666648ce8c743fd743f5f2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 4.527435366218612, Validation loss: 3.6120918705349876, Time: 45.131327867507935, LR:  1.51603258442109e-06\n",
      "Epoch 2, Training loss: 4.127560200775869, Validation loss: 3.5101857412429083, Time: 44.48086190223694, LR:  1.51603258442109e-06\n",
      "Epoch 3, Training loss: 4.013584167999629, Validation loss: 3.4502882730393183, Time: 44.59047508239746, LR:  1.1191416425799712e-06\n",
      "Epoch 4, Training loss: 3.9900497318019528, Validation loss: 3.4154942603338334, Time: 44.627875328063965, LR:  1.1191416425799712e-06\n",
      "Epoch 5, Training loss: 3.8709760474030084, Validation loss: 3.373841467357817, Time: 41.72936201095581, LR:  8.261550767623279e-07\n",
      "Epoch 6, Training loss: 3.8579395677916395, Validation loss: 3.346209685007731, Time: 32.453065156936646, LR:  8.261550767623279e-07\n",
      "Epoch 7, Training loss: 3.8074958945167134, Validation loss: 3.3328864120301747, Time: 32.57496976852417, LR:  6.098711591918945e-07\n",
      "Epoch 8, Training loss: 3.757411292318762, Validation loss: 3.316330217179798, Time: 32.59873175621033, LR:  6.098711591918945e-07\n",
      "Epoch 9, Training loss: 3.743559019100031, Validation loss: 3.3044964585985457, Time: 32.612971782684326, LR:  4.5020945979258006e-07\n",
      "Epoch 10, Training loss: 3.7425004786993625, Validation loss: 3.29518030938648, Time: 32.76325559616089, LR:  4.5020945979258006e-07\n",
      "Epoch 11, Training loss: 3.770058485177847, Validation loss: 3.288146450406029, Time: 32.55994892120361, LR:  3.323465204606458e-07\n",
      "Epoch 12, Training loss: 3.694562181213198, Validation loss: 3.2822379157656716, Time: 32.61668515205383, LR:  3.323465204606458e-07\n",
      "Epoch 13, Training loss: 3.6761624382797784, Validation loss: 3.2777103583017984, Time: 32.669546127319336, LR:  2.4533960195591356e-07\n",
      "Epoch 14, Training loss: 3.682681516782772, Validation loss: 3.273634013675508, Time: 32.743446350097656, LR:  2.4533960195591356e-07\n",
      "Epoch 15, Training loss: 3.6723218599014733, Validation loss: 3.268792890367054, Time: 32.60221552848816, LR:  1.8111072805714413e-07\n",
      "Epoch 16, Training loss: 3.66362577570966, Validation loss: 3.2651901699247814, Time: 32.5830934047699, LR:  1.8111072805714413e-07\n",
      "Epoch 17, Training loss: 3.6525146157078487, Validation loss: 3.262113468987601, Time: 32.73762273788452, LR:  1.336967026761665e-07\n",
      "Epoch 18, Training loss: 3.669574446226718, Validation loss: 3.2607289041791643, Time: 36.49718189239502, LR:  1.336967026761665e-07\n",
      "Epoch 19, Training loss: 3.6358344223372328, Validation loss: 3.2581515652792796, Time: 41.23122215270996, LR:  9.869546933100175e-08\n",
      "Epoch 20, Training loss: 3.6479490769685374, Validation loss: 3.256521020616804, Time: 41.21716618537903, LR:  9.869546933100175e-08\n",
      "[I 2024-08-24 18:53:10,765] Trial 0 finished with value: 3.256521020616804 and parameters: {'lr': 1.51603258442109e-06, 'weight_decay': 9.489674063306985e-05, 'step_size': 2, 'gamma': 0.7382042141312715}. Best is trial 0 with value: 3.256521020616804.\n",
      "Epoch 1, Training loss: 3.855241207681464, Validation loss: 3.129297347295852, Time: 41.061907052993774, LR:  7.084485032823294e-05\n",
      "Epoch 2, Training loss: 3.072200885891209, Validation loss: 3.106014473097665, Time: 41.481796741485596, LR:  7.084485032823294e-05\n",
      "Epoch 3, Training loss: 2.657730453113127, Validation loss: 3.1583973339625766, Time: 41.40025210380554, LR:  7.084485032823294e-05\n",
      "Epoch 4, Training loss: 2.2608710310515567, Validation loss: 3.2242027691432407, Time: 41.76465940475464, LR:  5.2301879144801676e-05\n",
      "Early stopping triggered at epoch 4\n",
      "[I 2024-08-24 18:55:59,549] Trial 1 finished with value: 3.2242027691432407 and parameters: {'lr': 7.084485032823294e-05, 'weight_decay': 2.054555353641544e-05, 'step_size': 3, 'gamma': 0.7382594345598954}. Best is trial 1 with value: 3.2242027691432407.\n",
      "Epoch 1, Training loss: 4.042051443686852, Validation loss: 3.30555754616147, Time: 36.47016406059265, LR:  7.491094788554261e-06\n",
      "Epoch 2, Training loss: 3.672510185185269, Validation loss: 3.21381299836295, Time: 33.03788614273071, LR:  4.022972374564396e-06\n",
      "Epoch 3, Training loss: 3.545041010929988, Validation loss: 3.1867154779888334, Time: 32.834038734436035, LR:  2.1604728258460304e-06\n",
      "Epoch 4, Training loss: 3.488782233740451, Validation loss: 3.1796471164340065, Time: 32.67425346374512, LR:  1.1602472989202517e-06\n",
      "Epoch 5, Training loss: 3.4749766136767595, Validation loss: 3.174452986036028, Time: 32.761571645736694, LR:  6.230922132170697e-07\n",
      "Epoch 6, Training loss: 3.4464894053498667, Validation loss: 3.1696937197730657, Time: 33.50026512145996, LR:  3.3462168499168535e-07\n",
      "Epoch 7, Training loss: 3.452039492906198, Validation loss: 3.169558048248291, Time: 41.016902685165405, LR:  1.7970321196690444e-07\n",
      "Epoch 8, Training loss: 3.4466418472267466, Validation loss: 3.1695249534788585, Time: 41.1343138217926, LR:  9.650672935923029e-08\n",
      "Epoch 9, Training loss: 3.4451716503448035, Validation loss: 3.169314770471482, Time: 40.83168292045593, LR:  5.1827392007500554e-08\n",
      "Epoch 10, Training loss: 3.4295278371438473, Validation loss: 3.1692224116552445, Time: 40.701071977615356, LR:  2.7833070088829253e-08\n",
      "Epoch 11, Training loss: 3.428958240345385, Validation loss: 3.169119562421526, Time: 40.60467743873596, LR:  1.4947304129398764e-08\n",
      "Epoch 12, Training loss: 3.436780949316081, Validation loss: 3.169095345905849, Time: 41.35625600814819, LR:  8.027210078647102e-09\n",
      "Epoch 13, Training loss: 3.427728480135901, Validation loss: 3.1690671898069835, Time: 41.46586775779724, LR:  4.310884497225084e-09\n",
      "Epoch 14, Training loss: 3.4475724076378276, Validation loss: 3.1690617970057895, Time: 35.19283151626587, LR:  2.3150914161134855e-09\n",
      "Epoch 15, Training loss: 3.4399233795482025, Validation loss: 3.1690587316240584, Time: 33.012017250061035, LR:  1.2432827343001998e-09\n",
      "Epoch 16, Training loss: 3.4301911775882425, Validation loss: 3.1690585499718074, Time: 33.16740155220032, LR:  6.676850627367228e-10\n",
      "Epoch 17, Training loss: 3.4381098796630045, Validation loss: 3.1690582093738375, Time: 33.318002223968506, LR:  3.585695600065327e-10\n",
      "Epoch 18, Training loss: 3.4331402242536377, Validation loss: 3.169057948248727, Time: 33.07324433326721, LR:  1.925640343611763e-10\n",
      "Epoch 19, Training loss: 3.435307959833089, Validation loss: 3.1690579709552584, Time: 39.75117325782776, LR:  1.034134278681568e-10\n",
      "Epoch 20, Training loss: 3.42577514845944, Validation loss: 3.169058016368321, Time: 40.951523542404175, LR:  5.55365237279148e-11\n",
      "Early stopping triggered at epoch 20\n",
      "[I 2024-08-24 19:08:19,079] Trial 2 finished with value: 3.169058016368321 and parameters: {'lr': 7.491094788554261e-06, 'weight_decay': 5.6515499991440486e-05, 'step_size': 1, 'gamma': 0.5370339700828705}. Best is trial 2 with value: 3.169058016368321.\n",
      "Epoch 1, Training loss: 3.9946207774461375, Validation loss: 3.257403884615217, Time: 40.79689002037048, LR:  1.182790135326749e-05\n",
      "Epoch 2, Training loss: 3.5487225951527703, Validation loss: 3.145613670349121, Time: 41.022570848464966, LR:  1.182790135326749e-05\n",
      "Epoch 3, Training loss: 3.3378306034754015, Validation loss: 3.090529033115932, Time: 41.23426389694214, LR:  7.266420675164005e-06\n",
      "Epoch 4, Training loss: 3.250645479506995, Validation loss: 3.083417688097273, Time: 41.75913214683533, LR:  7.266420675164005e-06\n",
      "Epoch 5, Training loss: 3.153383894784916, Validation loss: 3.058337699799311, Time: 41.758100509643555, LR:  4.464094504293827e-06\n",
      "Epoch 6, Training loss: 3.095953701515875, Validation loss: 3.049679302033924, Time: 35.145742416381836, LR:  4.464094504293827e-06\n",
      "Epoch 7, Training loss: 3.0287212598958666, Validation loss: 3.049498892965771, Time: 33.43713927268982, LR:  2.7424973909615497e-06\n",
      "Epoch 8, Training loss: 2.9959408573850372, Validation loss: 3.036403082665943, Time: 33.03283929824829, LR:  2.7424973909615497e-06\n",
      "Epoch 9, Training loss: 2.9564164256203105, Validation loss: 3.035755543481736, Time: 33.29105496406555, LR:  1.6848415579456236e-06\n",
      "Epoch 10, Training loss: 2.938290093777448, Validation loss: 3.031379228546506, Time: 33.43106698989868, LR:  1.6848415579456236e-06\n",
      "Epoch 11, Training loss: 2.918287685636938, Validation loss: 3.028009545235407, Time: 39.68310356140137, LR:  1.035075214560317e-06\n",
      "Epoch 12, Training loss: 2.914648654898243, Validation loss: 3.0279542548315868, Time: 40.655701637268066, LR:  1.035075214560317e-06\n",
      "Epoch 13, Training loss: 2.886473148884858, Validation loss: 3.0272748981203352, Time: 40.67272901535034, LR:  6.358940368870366e-07\n",
      "Epoch 14, Training loss: 2.8872688162256273, Validation loss: 3.028277698017302, Time: 40.25384545326233, LR:  6.358940368870366e-07\n",
      "Epoch 15, Training loss: 2.8803231624456553, Validation loss: 3.0275032009397234, Time: 41.13634467124939, LR:  3.906587854296732e-07\n",
      "Early stopping triggered at epoch 15\n",
      "[I 2024-08-24 19:17:59,795] Trial 3 finished with value: 3.0275032009397234 and parameters: {'lr': 1.182790135326749e-05, 'weight_decay': 1.4492559827866964e-05, 'step_size': 2, 'gamma': 0.614345728640748}. Best is trial 3 with value: 3.0275032009397234.\n",
      "Epoch 1, Training loss: 4.56362885130933, Validation loss: 3.6260088171277727, Time: 40.892858028411865, LR:  1.3352842855003652e-06\n",
      "Epoch 2, Training loss: 4.24252512045866, Validation loss: 3.5376379716963995, Time: 40.9163863658905, LR:  1.3352842855003652e-06\n",
      "Epoch 3, Training loss: 4.108500000993176, Validation loss: 3.4765971842266263, Time: 36.54378342628479, LR:  1.06426006014257e-06\n",
      "Epoch 4, Training loss: 3.9786349612580247, Validation loss: 3.428215708051409, Time: 32.95068669319153, LR:  1.06426006014257e-06\n",
      "Epoch 5, Training loss: 3.913031585117769, Validation loss: 3.3911826383499872, Time: 33.1346640586853, LR:  8.482459412680302e-07\n",
      "Epoch 6, Training loss: 3.8496487119494107, Validation loss: 3.3698454470861527, Time: 33.27698493003845, LR:  8.482459412680302e-07\n",
      "Epoch 7, Training loss: 3.838625614459698, Validation loss: 3.344560055505662, Time: 33.01316499710083, LR:  6.760764627221832e-07\n",
      "Epoch 8, Training loss: 3.795205478132124, Validation loss: 3.3222131842658635, Time: 36.400261878967285, LR:  6.760764627221832e-07\n",
      "Epoch 9, Training loss: 3.7329294836732765, Validation loss: 3.3088700430733815, Time: 41.395301818847656, LR:  5.388524261768449e-07\n",
      "Epoch 10, Training loss: 3.709547219897163, Validation loss: 3.293698242732457, Time: 41.00800275802612, LR:  5.388524261768449e-07\n",
      "Epoch 11, Training loss: 3.6975433163388947, Validation loss: 3.283512478783017, Time: 40.68294620513916, LR:  4.294809140781892e-07\n",
      "Epoch 12, Training loss: 3.684930342894334, Validation loss: 3.2773870740618025, Time: 41.30654430389404, LR:  4.294809140781892e-07\n",
      "Epoch 13, Training loss: 3.6743232315108623, Validation loss: 3.2709806760152182, Time: 41.60039949417114, LR:  3.423086667088726e-07\n",
      "Epoch 14, Training loss: 3.6577395348859256, Validation loss: 3.2653103783017112, Time: 40.87924361228943, LR:  3.423086667088726e-07\n",
      "Epoch 15, Training loss: 3.6607066773803982, Validation loss: 3.2604771795726957, Time: 39.66293811798096, LR:  2.728298731400056e-07\n",
      "Epoch 16, Training loss: 3.654990378921554, Validation loss: 3.2567783196767173, Time: 32.805155754089355, LR:  2.728298731400056e-07\n",
      "Epoch 17, Training loss: 3.6431178179012953, Validation loss: 3.252651032947359, Time: 32.82759070396423, LR:  2.1745327219803687e-07\n",
      "Epoch 18, Training loss: 3.6203719730207906, Validation loss: 3.2490962459927513, Time: 32.83549737930298, LR:  2.1745327219803687e-07\n",
      "Epoch 19, Training loss: 3.6366628182710277, Validation loss: 3.246235041391282, Time: 32.84099197387695, LR:  1.7331652522291145e-07\n",
      "Epoch 20, Training loss: 3.614431014427772, Validation loss: 3.243229877381098, Time: 32.881397008895874, LR:  1.7331652522291145e-07\n",
      "[I 2024-08-24 19:30:21,583] Trial 4 finished with value: 3.243229877381098 and parameters: {'lr': 1.3352842855003652e-06, 'weight_decay': 0.0003220236241729004, 'step_size': 2, 'gamma': 0.797028821277384}. Best is trial 3 with value: 3.0275032009397234.\n",
      "Epoch 1, Training loss: 4.094377761761818, Validation loss: 3.349819399061657, Time: 44.50822591781616, LR:  5.051404798056241e-06\n",
      "Epoch 2, Training loss: 3.711107599664722, Validation loss: 3.23228044736953, Time: 44.292261362075806, LR:  5.051404798056241e-06\n",
      "Epoch 3, Training loss: 3.5568579710446873, Validation loss: 3.1773187205905007, Time: 44.80397439002991, LR:  3.5952762065990255e-06\n",
      "Epoch 4, Training loss: 3.4516464090911594, Validation loss: 3.1532765343075706, Time: 44.52539849281311, LR:  3.5952762065990255e-06\n",
      "Epoch 5, Training loss: 3.3972829806028737, Validation loss: 3.144094716934931, Time: 44.77986288070679, LR:  2.5588943112836556e-06\n",
      "Epoch 6, Training loss: 3.329977161080174, Validation loss: 3.114625817253476, Time: 44.9406635761261, LR:  2.5588943112836556e-06\n",
      "Epoch 7, Training loss: 3.2787100090783023, Validation loss: 3.1063074270884194, Time: 44.74719214439392, LR:  1.8212620449859457e-06\n",
      "Epoch 8, Training loss: 3.2460831530700776, Validation loss: 3.0933231512705484, Time: 44.83577919006348, LR:  1.8212620449859457e-06\n",
      "Epoch 9, Training loss: 3.213884359986119, Validation loss: 3.0889356363387335, Time: 44.8239860534668, LR:  1.296261210117129e-06\n",
      "Epoch 10, Training loss: 3.195070339377815, Validation loss: 3.0755303019569036, Time: 44.74760437011719, LR:  1.296261210117129e-06\n",
      "Epoch 11, Training loss: 3.1756903053035397, Validation loss: 3.0755316416422525, Time: 44.87217164039612, LR:  9.225982221944839e-07\n",
      "Epoch 12, Training loss: 3.1608216261722633, Validation loss: 3.0762980551946733, Time: 44.6467502117157, LR:  9.225982221944839e-07\n",
      "Early stopping triggered at epoch 12\n",
      "[I 2024-08-24 19:39:21,592] Trial 5 finished with value: 3.0762980551946733 and parameters: {'lr': 5.051404798056241e-06, 'weight_decay': 4.262018992044036e-05, 'step_size': 2, 'gamma': 0.7117378927902338}. Best is trial 3 with value: 3.0275032009397234.\n",
      "Epoch 1, Training loss: 4.085485830814881, Validation loss: 3.3458163624718074, Time: 44.90885543823242, LR:  6.0268237703205224e-06\n",
      "Epoch 2, Training loss: 3.712209180967342, Validation loss: 3.2182573136829196, Time: 44.982665061950684, LR:  6.0268237703205224e-06\n",
      "Epoch 3, Training loss: 3.5721805659976935, Validation loss: 3.1669203213282993, Time: 44.3937668800354, LR:  5.883081649864206e-06\n",
      "Epoch 4, Training loss: 3.4676714973336846, Validation loss: 3.133854945500692, Time: 44.35852837562561, LR:  5.883081649864206e-06\n",
      "Epoch 5, Training loss: 3.343770375618568, Validation loss: 3.118553706577846, Time: 44.5045108795166, LR:  5.742767835590497e-06\n",
      "Epoch 6, Training loss: 3.268000104018217, Validation loss: 3.079632838567098, Time: 44.50221085548401, LR:  5.742767835590497e-06\n",
      "Epoch 7, Training loss: 3.189001503780749, Validation loss: 3.059899534497942, Time: 44.28301811218262, LR:  5.605800561046777e-06\n",
      "Epoch 8, Training loss: 3.1007758759887967, Validation loss: 3.039708211308434, Time: 44.445117712020874, LR:  5.605800561046777e-06\n",
      "Epoch 9, Training loss: 3.0353947609839356, Validation loss: 3.035246281396775, Time: 44.68115258216858, LR:  5.472100009942523e-06\n",
      "Epoch 10, Training loss: 2.95986054917059, Validation loss: 3.026611566543579, Time: 35.24833822250366, LR:  5.472100009942523e-06\n",
      "Epoch 11, Training loss: 2.9082424291492215, Validation loss: 3.031787009466262, Time: 32.60124373435974, LR:  5.341588269637176e-06\n",
      "Epoch 12, Training loss: 2.878666283110895, Validation loss: 3.024768976938157, Time: 32.74929237365723, LR:  5.341588269637176e-06\n",
      "Epoch 13, Training loss: 2.8006299004046875, Validation loss: 3.01531990369161, Time: 32.632179260253906, LR:  5.214189285737336e-06\n",
      "Epoch 14, Training loss: 2.7606257069745714, Validation loss: 3.015506381080264, Time: 32.75008797645569, LR:  5.214189285737336e-06\n",
      "Epoch 15, Training loss: 2.7007751863383684, Validation loss: 3.028179707981291, Time: 32.687777280807495, LR:  5.089828817776842e-06\n",
      "Early stopping triggered at epoch 15\n",
      "[I 2024-08-24 19:49:25,007] Trial 6 finished with value: 3.028179707981291 and parameters: {'lr': 6.0268237703205224e-06, 'weight_decay': 2.3462885551285177e-05, 'step_size': 2, 'gamma': 0.9761496061716316}. Best is trial 3 with value: 3.0275032009397234.\n",
      "Epoch 1, Training loss: 3.856273212376431, Validation loss: 3.1438231354668025, Time: 32.582852602005005, LR:  7.461271503723562e-05\n",
      "Epoch 2, Training loss: 2.9820454579133253, Validation loss: 3.0375549906776067, Time: 32.70315480232239, LR:  4.784814867614248e-05\n",
      "Epoch 3, Training loss: 2.5876278023748003, Validation loss: 3.0641649734406244, Time: 32.55953931808472, LR:  3.068438577247435e-05\n",
      "Epoch 4, Training loss: 2.338958424576641, Validation loss: 3.0847671259017218, Time: 32.74003744125366, LR:  1.967749131960632e-05\n",
      "Early stopping triggered at epoch 4\n",
      "[I 2024-08-24 19:51:38,715] Trial 7 finished with value: 3.0847671259017218 and parameters: {'lr': 7.461271503723562e-05, 'weight_decay': 3.2520774838256884e-05, 'step_size': 1, 'gamma': 0.6412867921005658}. Best is trial 3 with value: 3.0275032009397234.\n",
      "Epoch 1, Training loss: 4.481310861350517, Validation loss: 3.5198363690149215, Time: 32.51538372039795, LR:  3.2659507840470107e-06\n",
      "Epoch 2, Training loss: 4.019998506681453, Validation loss: 3.3766051701136996, Time: 38.38631725311279, LR:  3.2659507840470107e-06\n",
      "Epoch 3, Training loss: 3.763454064815002, Validation loss: 3.288559050787063, Time: 44.49792528152466, LR:  3.2659507840470107e-06\n",
      "Epoch 4, Training loss: 3.655944793887392, Validation loss: 3.2408385617392406, Time: 44.153687715530396, LR:  3.1288045286802788e-06\n",
      "Epoch 5, Training loss: 3.5756180363999315, Validation loss: 3.187206540788923, Time: 44.04487466812134, LR:  3.1288045286802788e-06\n",
      "Epoch 6, Training loss: 3.484913487406172, Validation loss: 3.1557782831646146, Time: 44.57000160217285, LR:  3.1288045286802788e-06\n",
      "Epoch 7, Training loss: 3.4386896311178714, Validation loss: 3.1366200220017206, Time: 44.37142252922058, LR:  2.9974174217529516e-06\n",
      "Epoch 8, Training loss: 3.3619634964056972, Validation loss: 3.134925603866577, Time: 44.34361433982849, LR:  2.9974174217529516e-06\n",
      "Epoch 9, Training loss: 3.3611129226063836, Validation loss: 3.114868334361485, Time: 44.77588224411011, LR:  2.9974174217529516e-06\n",
      "Epoch 10, Training loss: 3.289936682881688, Validation loss: 3.0975654352278936, Time: 44.90118384361267, LR:  2.8715476207833775e-06\n",
      "Epoch 11, Training loss: 3.2054656828648946, Validation loss: 3.084561745325724, Time: 44.58773708343506, LR:  2.8715476207833775e-06\n",
      "Epoch 12, Training loss: 3.170843471436811, Validation loss: 3.07517417271932, Time: 44.780579805374146, LR:  2.8715476207833775e-06\n",
      "Epoch 13, Training loss: 3.130223734138985, Validation loss: 3.0670532953171503, Time: 44.650192975997925, LR:  2.7509634389208194e-06\n",
      "Epoch 14, Training loss: 3.1070239783744134, Validation loss: 3.0545558475312733, Time: 44.50149321556091, LR:  2.7509634389208194e-06\n",
      "Epoch 15, Training loss: 3.0660418356664083, Validation loss: 3.0492619219280424, Time: 44.30061411857605, LR:  2.7509634389208194e-06\n",
      "Epoch 16, Training loss: 3.024344397719795, Validation loss: 3.04130714847928, Time: 44.83540606498718, LR:  2.635442918482583e-06\n",
      "Epoch 17, Training loss: 2.9822652586818448, Validation loss: 3.0462606634412492, Time: 44.175353050231934, LR:  2.635442918482583e-06\n",
      "Epoch 18, Training loss: 2.9574513371879534, Validation loss: 3.033339193889073, Time: 44.30620265007019, LR:  2.635442918482583e-06\n",
      "Epoch 19, Training loss: 2.9181360972703563, Validation loss: 3.0308879840941656, Time: 44.23231267929077, LR:  2.5247734223994924e-06\n",
      "Epoch 20, Training loss: 2.8828363799484524, Validation loss: 3.0396808556147983, Time: 44.171289682388306, LR:  2.5247734223994924e-06\n",
      "[I 2024-08-24 20:06:13,232] Trial 8 finished with value: 3.0396808556147983 and parameters: {'lr': 3.2659507840470107e-06, 'weight_decay': 0.0006465987374258237, 'step_size': 3, 'gamma': 0.9580072498224279}. Best is trial 3 with value: 3.0275032009397234.\n",
      "Epoch 1, Training loss: 3.906572855435885, Validation loss: 3.203643253871373, Time: 44.15220093727112, LR:  1.586862472625103e-05\n",
      "Epoch 2, Training loss: 3.436541043089692, Validation loss: 3.1355981599716913, Time: 44.27201557159424, LR:  1.3359833187774068e-05\n",
      "Epoch 3, Training loss: 3.2150381154562595, Validation loss: 3.052807864688692, Time: 44.17346405982971, LR:  1.1247675578960937e-05\n",
      "Epoch 4, Training loss: 3.0617209806950134, Validation loss: 3.080084108171009, Time: 44.24519658088684, LR:  9.469445026104596e-06\n",
      "Epoch 5, Training loss: 2.970833151298162, Validation loss: 3.038671238081796, Time: 35.919711112976074, LR:  7.972348461947801e-06\n",
      "Epoch 6, Training loss: 2.8671838792823476, Validation loss: 3.0162567297617593, Time: 32.73377752304077, LR:  6.711939276642824e-06\n",
      "Epoch 7, Training loss: 2.7942298855301897, Validation loss: 3.0183055854979015, Time: 32.62012243270874, LR:  5.650797762838125e-06\n",
      "Epoch 8, Training loss: 2.7195774598939884, Validation loss: 3.0216321774891446, Time: 32.54902243614197, LR:  4.7574201792343766e-06\n",
      "Early stopping triggered at epoch 8\n",
      "[I 2024-08-24 20:11:26,833] Trial 9 finished with value: 3.0216321774891446 and parameters: {'lr': 1.586862472625103e-05, 'weight_decay': 0.0002337402890268648, 'step_size': 1, 'gamma': 0.8419023965998302}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.8414746939077884, Validation loss: 3.1851912225995744, Time: 32.6872878074646, LR:  2.3174066363433326e-05\n",
      "Epoch 2, Training loss: 3.320567669953115, Validation loss: 3.0734696047646657, Time: 32.54041266441345, LR:  1.9657497880574843e-05\n",
      "Epoch 3, Training loss: 3.0562278406154473, Validation loss: 3.049200966244652, Time: 32.72193717956543, LR:  1.6674554084066033e-05\n",
      "Epoch 4, Training loss: 2.942976221530395, Validation loss: 3.0305610497792563, Time: 32.751285791397095, LR:  1.4144259640348108e-05\n",
      "Epoch 5, Training loss: 2.728844979810997, Validation loss: 3.049030513990493, Time: 32.55577087402344, LR:  1.1997926886977742e-05\n",
      "Epoch 6, Training loss: 2.620705195432584, Validation loss: 3.0460481416611445, Time: 32.783278942108154, LR:  1.017729122948429e-05\n",
      "Early stopping triggered at epoch 6\n",
      "[I 2024-08-24 20:14:45,690] Trial 10 finished with value: 3.0460481416611445 and parameters: {'lr': 2.3174066363433326e-05, 'weight_decay': 0.00023438675595603125, 'step_size': 1, 'gamma': 0.8482541463501061}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.9174854614325527, Validation loss: 3.1756003130049932, Time: 32.44788312911987, LR:  1.9498248067490606e-05\n",
      "Epoch 2, Training loss: 3.402049206417693, Validation loss: 3.151751983733404, Time: 32.52391695976257, LR:  1.1280149696512102e-05\n",
      "Epoch 3, Training loss: 3.2446776826000776, Validation loss: 3.108586935769944, Time: 38.15769910812378, LR:  6.525805638295886e-06\n",
      "Epoch 4, Training loss: 3.1422374911562225, Validation loss: 3.107248828524635, Time: 41.10203289985657, LR:  3.7753168508022812e-06\n",
      "Epoch 5, Training loss: 3.0854015350341797, Validation loss: 3.0865801572799683, Time: 41.393720388412476, LR:  2.18410080133395e-06\n",
      "Epoch 6, Training loss: 3.059609051992202, Validation loss: 3.0814460856573924, Time: 40.60355806350708, LR:  1.2635485970863299e-06\n",
      "Epoch 7, Training loss: 3.0420025319037354, Validation loss: 3.0799821671985446, Time: 41.25201725959778, LR:  7.309896394084598e-07\n",
      "Epoch 8, Training loss: 3.0459974348192382, Validation loss: 3.0751524595987227, Time: 41.129605293273926, LR:  4.228929968777463e-07\n",
      "Epoch 9, Training loss: 3.0284315904922035, Validation loss: 3.0738648176193237, Time: 41.28957390785217, LR:  2.446525602646891e-07\n",
      "Epoch 10, Training loss: 3.035494195639029, Validation loss: 3.073723179953439, Time: 37.557347536087036, LR:  1.4153669057180134e-07\n",
      "Epoch 11, Training loss: 3.0162248801902907, Validation loss: 3.074090588660467, Time: 33.007795095443726, LR:  8.188197481499712e-08\n",
      "Epoch 12, Training loss: 3.0214957217493, Validation loss: 3.074046884264265, Time: 32.61349105834961, LR:  4.737045760019774e-08\n",
      "Early stopping triggered at epoch 12\n",
      "[I 2024-08-24 20:22:22,123] Trial 11 finished with value: 3.074046884264265 and parameters: {'lr': 1.9498248067490606e-05, 'weight_decay': 1.1414050000754355e-05, 'step_size': 1, 'gamma': 0.5785211911073936}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.8947869800251618, Validation loss: 3.2084610008058094, Time: 32.91574192047119, LR:  1.8413799912307516e-05\n",
      "Epoch 2, Training loss: 3.47738110172678, Validation loss: 3.081390869049799, Time: 32.78161597251892, LR:  1.8413799912307516e-05\n",
      "Epoch 3, Training loss: 3.166260362376828, Validation loss: 3.0290321054912748, Time: 34.05707144737244, LR:  1.8413799912307516e-05\n",
      "Epoch 4, Training loss: 2.9625827567817193, Validation loss: 3.0174550101870583, Time: 40.926647424697876, LR:  1.584995461490181e-05\n",
      "Epoch 5, Training loss: 2.8179476874819875, Validation loss: 3.0405493747620356, Time: 40.7459614276886, LR:  1.584995461490181e-05\n",
      "Epoch 6, Training loss: 2.695252122963674, Validation loss: 3.0222967636017573, Time: 40.63199758529663, LR:  1.584995461490181e-05\n",
      "Early stopping triggered at epoch 6\n",
      "[I 2024-08-24 20:26:08,151] Trial 12 finished with value: 3.0222967636017573 and parameters: {'lr': 1.8413799912307516e-05, 'weight_decay': 0.00017507244097354638, 'step_size': 3, 'gamma': 0.8607650072437212}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.837240917442818, Validation loss: 3.1858688990275064, Time: 40.281920433044434, LR:  3.121710386583961e-05\n",
      "Epoch 2, Training loss: 3.251644248793111, Validation loss: 3.0396326837085543, Time: 41.12486267089844, LR:  3.121710386583961e-05\n",
      "Epoch 3, Training loss: 2.9201266723271657, Validation loss: 3.0253890241895403, Time: 40.72509980201721, LR:  3.121710386583961e-05\n",
      "Epoch 4, Training loss: 2.6494753099757538, Validation loss: 3.0391320273989724, Time: 40.87618660926819, LR:  2.7414895755411254e-05\n",
      "Epoch 5, Training loss: 2.4548096353485738, Validation loss: 3.133146058945429, Time: 34.175209045410156, LR:  2.7414895755411254e-05\n",
      "Early stopping triggered at epoch 5\n",
      "[I 2024-08-24 20:29:28,828] Trial 13 finished with value: 3.133146058945429 and parameters: {'lr': 3.121710386583961e-05, 'weight_decay': 0.00020228643726966975, 'step_size': 3, 'gamma': 0.8782011256787644}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.923876615671011, Validation loss: 3.311262085324242, Time: 33.10334753990173, LR:  3.5314752589799536e-05\n",
      "Epoch 2, Training loss: 3.3777899742126465, Validation loss: 3.078609284900484, Time: 32.933356285095215, LR:  3.5314752589799536e-05\n",
      "Epoch 3, Training loss: 2.9729584956310204, Validation loss: 3.08781665847415, Time: 33.115487813949585, LR:  3.5314752589799536e-05\n",
      "Epoch 4, Training loss: 2.704317391976802, Validation loss: 3.057268506004697, Time: 33.68475317955017, LR:  3.132015542274855e-05\n",
      "Epoch 5, Training loss: 2.492470600901271, Validation loss: 3.1190538803736367, Time: 41.633758783340454, LR:  3.132015542274855e-05\n",
      "Epoch 6, Training loss: 2.310279630345, Validation loss: 3.1222764423915317, Time: 41.64264464378357, LR:  3.132015542274855e-05\n",
      "Early stopping triggered at epoch 6\n",
      "[I 2024-08-24 20:33:08,751] Trial 14 finished with value: 3.1222764423915317 and parameters: {'lr': 3.5314752589799536e-05, 'weight_decay': 0.0007130078325843983, 'step_size': 3, 'gamma': 0.8868858798630008}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.9833889346150957, Validation loss: 3.2487220537094843, Time: 40.90051794052124, LR:  1.056599726324858e-05\n",
      "Epoch 2, Training loss: 3.56160114996532, Validation loss: 3.1511599676949635, Time: 41.7071328163147, LR:  8.594152955627325e-06\n",
      "Epoch 3, Training loss: 3.3782051745251085, Validation loss: 3.102787120001657, Time: 40.85012340545654, LR:  6.9902975729154545e-06\n",
      "Epoch 4, Training loss: 3.2657176862806963, Validation loss: 3.0721775804247176, Time: 41.796605348587036, LR:  5.6857563985886585e-06\n",
      "Epoch 5, Training loss: 3.16749135276975, Validation loss: 3.070481845310756, Time: 40.042017221450806, LR:  4.624670908052467e-06\n",
      "Epoch 6, Training loss: 3.1046147304173757, Validation loss: 3.048030217488607, Time: 33.19722032546997, LR:  3.7616069891942153e-06\n",
      "Epoch 7, Training loss: 3.0374878362791073, Validation loss: 3.0443793535232544, Time: 32.8612174987793, LR:  3.0596095208671747e-06\n",
      "Epoch 8, Training loss: 3.0046857694196984, Validation loss: 3.0327921765191213, Time: 33.342873334884644, LR:  2.4886200092334353e-06\n",
      "Epoch 9, Training loss: 2.9808271971680003, Validation loss: 3.040040146736872, Time: 32.764861822128296, LR:  2.0241895274929387e-06\n",
      "Epoch 10, Training loss: 2.9381889217704007, Validation loss: 3.0325016464505876, Time: 33.77114796638489, LR:  1.6464318489805052e-06\n",
      "Epoch 11, Training loss: 2.924970598263148, Validation loss: 3.032589083626157, Time: 41.51878356933594, LR:  1.3391719483376397e-06\n",
      "Epoch 12, Training loss: 2.905560628196897, Validation loss: 3.0290899730864025, Time: 41.095876693725586, LR:  1.0892534108380606e-06\n",
      "Epoch 13, Training loss: 2.8888510659601563, Validation loss: 3.03160598164513, Time: 41.29833745956421, LR:  8.859750941581166e-07\n",
      "Epoch 14, Training loss: 2.8894308932434174, Validation loss: 3.0282028799965266, Time: 41.394516944885254, LR:  7.206329212818802e-07\n",
      "Epoch 15, Training loss: 2.870487109091155, Validation loss: 3.024048084304446, Time: 41.4891254901886, LR:  5.861471847904755e-07\n",
      "Epoch 16, Training loss: 2.871962963476689, Validation loss: 3.0254920777820407, Time: 40.20824885368347, LR:  4.7675940425626325e-07\n",
      "Epoch 17, Training loss: 2.850307287901816, Validation loss: 3.024753133455912, Time: 41.073739767074585, LR:  3.877857566236331e-07\n",
      "Early stopping triggered at epoch 17\n",
      "[I 2024-08-24 20:44:10,947] Trial 15 finished with value: 3.024753133455912 and parameters: {'lr': 1.056599726324858e-05, 'weight_decay': 0.00013820840380906734, 'step_size': 1, 'gamma': 0.8133783060421691}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.9292841564268755, Validation loss: 3.2403239636194137, Time: 33.0792396068573, LR:  1.6174784948550492e-05\n",
      "Epoch 2, Training loss: 3.481252515809776, Validation loss: 3.113548925944737, Time: 32.68373990058899, LR:  1.6174784948550492e-05\n",
      "Epoch 3, Training loss: 3.229158672355336, Validation loss: 3.0659086136590865, Time: 32.9501850605011, LR:  1.6174784948550492e-05\n",
      "Epoch 4, Training loss: 3.0175843492767513, Validation loss: 3.0552767231350852, Time: 32.7065064907074, LR:  1.4996010367598271e-05\n",
      "Epoch 5, Training loss: 2.868077788127245, Validation loss: 3.0319947458448864, Time: 32.95354080200195, LR:  1.4996010367598271e-05\n",
      "Epoch 6, Training loss: 2.7360362770289357, Validation loss: 3.059132382983253, Time: 38.003281593322754, LR:  1.4996010367598271e-05\n",
      "Epoch 7, Training loss: 2.5915034268734725, Validation loss: 3.0603036483128867, Time: 41.46455693244934, LR:  1.390314169000853e-05\n",
      "Early stopping triggered at epoch 7\n",
      "[I 2024-08-24 20:48:17,541] Trial 16 finished with value: 3.0603036483128867 and parameters: {'lr': 1.6174784948550492e-05, 'weight_decay': 0.0004025436593554124, 'step_size': 3, 'gamma': 0.927122704586075}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.4441697978409085, Validation loss: 7.324576082683745, Time: 41.8776433467865, LR:  4.97950108469225e-05\n",
      "Epoch 2, Training loss: 5.161655351255067, Validation loss: 3.217327186039516, Time: 41.843151330947876, LR:  4.97950108469225e-05\n",
      "Epoch 3, Training loss: 3.030352412596257, Validation loss: 3.054103204182216, Time: 41.75068497657776, LR:  4.0202896794074235e-05\n",
      "Epoch 4, Training loss: 2.7091203194398146, Validation loss: 3.0956564914612543, Time: 42.122599840164185, LR:  4.0202896794074235e-05\n",
      "Epoch 5, Training loss: 2.60324229502819, Validation loss: 3.142327274594988, Time: 41.87316012382507, LR:  3.2458531148906765e-05\n",
      "Early stopping triggered at epoch 5\n",
      "[I 2024-08-24 20:51:50,492] Trial 17 finished with value: 3.142327274594988 and parameters: {'lr': 4.97950108469225e-05, 'weight_decay': 8.14863147802676e-05, 'step_size': 2, 'gamma': 0.807367969406897}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.361381269770966, Validation loss: 3.4963578837258473, Time: 33.18968915939331, LR:  3.5444830648805844e-06\n",
      "Epoch 2, Training loss: 3.9326156215554864, Validation loss: 3.3400927044096447, Time: 32.93102192878723, LR:  3.5444830648805844e-06\n",
      "Epoch 3, Training loss: 3.7696308356065016, Validation loss: 3.2697543643769764, Time: 33.307581424713135, LR:  3.5444830648805844e-06\n",
      "Epoch 4, Training loss: 3.6505495415636773, Validation loss: 3.223840827033633, Time: 33.35906267166138, LR:  2.363578858625691e-06\n",
      "Epoch 5, Training loss: 3.621038889038492, Validation loss: 3.2068214530036565, Time: 34.02608275413513, LR:  2.363578858625691e-06\n",
      "Epoch 6, Training loss: 3.612615633998397, Validation loss: 3.1878411656334285, Time: 44.55361008644104, LR:  2.363578858625691e-06\n",
      "Epoch 7, Training loss: 3.473797778406087, Validation loss: 3.169633944829305, Time: 44.0162878036499, LR:  1.576112769812468e-06\n",
      "Epoch 8, Training loss: 3.436790382368325, Validation loss: 3.150335493541899, Time: 44.68729543685913, LR:  1.576112769812468e-06\n",
      "Epoch 9, Training loss: 3.4302176759087826, Validation loss: 3.139786163965861, Time: 44.74983263015747, LR:  1.576112769812468e-06\n",
      "Epoch 10, Training loss: 3.3789864588065965, Validation loss: 3.1368498915717717, Time: 44.490342140197754, LR:  1.05100426588277e-06\n",
      "Epoch 11, Training loss: 3.36906916925893, Validation loss: 3.1287813640776134, Time: 44.505218505859375, LR:  1.05100426588277e-06\n",
      "Epoch 12, Training loss: 3.3516422735868825, Validation loss: 3.121849536895752, Time: 44.737597703933716, LR:  1.05100426588277e-06\n",
      "Epoch 13, Training loss: 3.3295440984195506, Validation loss: 3.117054814384097, Time: 44.72574234008789, LR:  7.008445005081781e-07\n",
      "Epoch 14, Training loss: 3.311020736158247, Validation loss: 3.114170415060861, Time: 44.579082012176514, LR:  7.008445005081781e-07\n",
      "Epoch 15, Training loss: 3.2960061758932984, Validation loss: 3.1114005020686557, Time: 44.712175369262695, LR:  7.008445005081781e-07\n",
      "Epoch 16, Training loss: 3.305056657311479, Validation loss: 3.108518327985491, Time: 44.96148490905762, LR:  4.673463560873356e-07\n",
      "Epoch 17, Training loss: 3.294535674287017, Validation loss: 3.1064960275377547, Time: 44.705942153930664, LR:  4.673463560873356e-07\n",
      "Epoch 18, Training loss: 3.264976604450384, Validation loss: 3.1056823276338124, Time: 44.884108543395996, LR:  4.673463560873356e-07\n",
      "Epoch 19, Training loss: 3.265185844968762, Validation loss: 3.1016641912006198, Time: 44.81656455993652, LR:  3.116420495412905e-07\n",
      "Epoch 20, Training loss: 3.2610053952629046, Validation loss: 3.101855436960856, Time: 44.83483099937439, LR:  3.116420495412905e-07\n",
      "[I 2024-08-24 21:05:50,667] Trial 18 finished with value: 3.101855436960856 and parameters: {'lr': 3.5444830648805844e-06, 'weight_decay': 0.00017368087711065055, 'step_size': 3, 'gamma': 0.666833164487223}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.991553450477194, Validation loss: 3.231960807527815, Time: 44.56459999084473, LR:  1.3194138733665458e-05\n",
      "Epoch 2, Training loss: 3.5315904984107385, Validation loss: 3.125559988475981, Time: 44.44151735305786, LR:  1.2114199392797735e-05\n",
      "Epoch 3, Training loss: 3.2836131555794257, Validation loss: 3.0798461777823314, Time: 44.32482314109802, LR:  1.1122653012129682e-05\n",
      "Epoch 4, Training loss: 3.1493517343814554, Validation loss: 3.0481555802481517, Time: 44.79111933708191, LR:  1.0212264633995452e-05\n",
      "Epoch 5, Training loss: 3.0107236056638187, Validation loss: 3.033316935811724, Time: 44.43359684944153, LR:  9.376391481512695e-06\n",
      "Epoch 6, Training loss: 2.896016354391561, Validation loss: 3.0254800660269603, Time: 44.40226078033447, LR:  8.608934488626471e-06\n",
      "Epoch 7, Training loss: 2.8037232139406827, Validation loss: 3.0383366970788863, Time: 36.146575927734375, LR:  7.904293797415713e-06\n",
      "Epoch 8, Training loss: 2.7510739677756497, Validation loss: 3.0364694197972617, Time: 32.704713344573975, LR:  7.2573278979420665e-06\n",
      "Early stopping triggered at epoch 8\n",
      "[I 2024-08-24 21:11:29,363] Trial 19 finished with value: 3.0364694197972617 and parameters: {'lr': 1.3194138733665458e-05, 'weight_decay': 0.00040562580499875744, 'step_size': 1, 'gamma': 0.9181500693097756}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.9674683209707045, Validation loss: 3.1923076198214577, Time: 32.45262885093689, LR:  3.098403126111365e-05\n",
      "Epoch 2, Training loss: 3.327641861678581, Validation loss: 3.0886006582350958, Time: 32.619895696640015, LR:  3.098403126111365e-05\n",
      "Epoch 3, Training loss: 3.0743829872481214, Validation loss: 3.059929058665321, Time: 32.53209590911865, LR:  2.4158090922170547e-05\n",
      "Epoch 4, Training loss: 2.835093722540951, Validation loss: 3.0478519825708297, Time: 32.58006501197815, LR:  2.4158090922170547e-05\n",
      "Epoch 5, Training loss: 2.6129948711959567, Validation loss: 3.0516038054511663, Time: 32.59203553199768, LR:  1.883594010364687e-05\n",
      "Epoch 6, Training loss: 2.4743255683656273, Validation loss: 3.0916011276699247, Time: 32.59703063964844, LR:  1.883594010364687e-05\n",
      "Early stopping triggered at epoch 6\n",
      "[I 2024-08-24 21:14:48,072] Trial 20 finished with value: 3.0916011276699247 and parameters: {'lr': 3.098403126111365e-05, 'weight_decay': 0.00011828604224262101, 'step_size': 2, 'gamma': 0.7796948924619126}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.034645184962707, Validation loss: 3.2916762488228932, Time: 32.460405588150024, LR:  8.819237329545027e-06\n",
      "Epoch 2, Training loss: 3.6273673885672757, Validation loss: 3.178072146006993, Time: 32.51566672325134, LR:  7.36752950180653e-06\n",
      "Epoch 3, Training loss: 3.4536180997035912, Validation loss: 3.1236991655258906, Time: 32.49192404747009, LR:  6.154782883339169e-06\n",
      "Epoch 4, Training loss: 3.3364683149834358, Validation loss: 3.1045833996364047, Time: 37.32310891151428, LR:  5.141662796430777e-06\n",
      "Epoch 5, Training loss: 3.2431189019299116, Validation loss: 3.0689495518094017, Time: 44.42697262763977, LR:  4.295309325007025e-06\n",
      "Epoch 6, Training loss: 3.163833228794075, Validation loss: 3.0702686309814453, Time: 44.18457889556885, LR:  3.5882715238151457e-06\n",
      "Epoch 7, Training loss: 3.1342836745391938, Validation loss: 3.0579429467519126, Time: 44.675626039505005, LR:  2.9976170641916712e-06\n",
      "Epoch 8, Training loss: 3.093669430744013, Validation loss: 3.0540425777435303, Time: 44.16875171661377, LR:  2.5041884383317933e-06\n",
      "Epoch 9, Training loss: 3.0418952879821055, Validation loss: 3.0468863305591403, Time: 44.77594304084778, LR:  2.091981597511233e-06\n",
      "Epoch 10, Training loss: 3.0139611677305234, Validation loss: 3.0406371582122076, Time: 44.715571641922, LR:  1.7476268707801617e-06\n",
      "Epoch 11, Training loss: 3.002206527975184, Validation loss: 3.0391800006230674, Time: 44.86485028266907, LR:  1.459955328051809e-06\n",
      "Epoch 12, Training loss: 2.9811614250995704, Validation loss: 3.0353902124223255, Time: 44.913902759552, LR:  1.21963652284389e-06\n",
      "Epoch 13, Training loss: 2.9524494695240224, Validation loss: 3.035226129350208, Time: 44.64973735809326, LR:  1.0188758650853378e-06\n",
      "Epoch 14, Training loss: 2.9388500603698415, Validation loss: 3.0298113766170682, Time: 44.67245650291443, LR:  8.511618084646931e-07\n",
      "Epoch 15, Training loss: 2.9348595107095483, Validation loss: 3.031617147581918, Time: 44.5976619720459, LR:  7.110546524999953e-07\n",
      "Epoch 16, Training loss: 2.9180342459819726, Validation loss: 3.0334598280134655, Time: 44.83838629722595, LR:  5.940101092574594e-07\n",
      "Early stopping triggered at epoch 16\n",
      "[I 2024-08-24 21:26:01,105] Trial 21 finished with value: 3.0334598280134655 and parameters: {'lr': 8.819237329545027e-06, 'weight_decay': 0.000141136835294402, 'step_size': 1, 'gamma': 0.8353930421086209}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.058154551940556, Validation loss: 3.2425570374443415, Time: 44.311694622039795, LR:  1.1564848678655153e-05\n",
      "Epoch 2, Training loss: 3.5650525819620436, Validation loss: 3.1699409825461253, Time: 44.68693566322327, LR:  9.88283800155611e-06\n",
      "Epoch 3, Training loss: 3.390314670709463, Validation loss: 3.1047482831137523, Time: 44.2674355506897, LR:  8.445461733128308e-06\n",
      "Epoch 4, Training loss: 3.2356339855306953, Validation loss: 3.0871927056993758, Time: 44.41614246368408, LR:  7.2171398412585465e-06\n",
      "Epoch 5, Training loss: 3.134440299321914, Validation loss: 3.0494276114872525, Time: 44.39291214942932, LR:  6.1674671124213014e-06\n",
      "Epoch 6, Training loss: 3.057302872457448, Validation loss: 3.0341659103121077, Time: 44.31744575500488, LR:  5.270460517523411e-06\n",
      "Epoch 7, Training loss: 3.043567156650611, Validation loss: 3.063562523751032, Time: 44.37913537025452, LR:  4.50391604210238e-06\n",
      "Epoch 8, Training loss: 2.9640922059674235, Validation loss: 3.070919786180769, Time: 44.513604402542114, LR:  3.848859060202049e-06\n",
      "Early stopping triggered at epoch 8\n",
      "[I 2024-08-24 21:31:59,219] Trial 22 finished with value: 3.070919786180769 and parameters: {'lr': 1.1564848678655153e-05, 'weight_decay': 7.338860367320165e-05, 'step_size': 1, 'gamma': 0.8545583497168041}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.841749707622641, Validation loss: 3.1675213972727456, Time: 44.194560289382935, LR:  2.0582664712018654e-05\n",
      "Epoch 2, Training loss: 3.3532628350003937, Validation loss: 3.1012765112377347, Time: 44.34288740158081, LR:  1.626734385202616e-05\n",
      "Epoch 3, Training loss: 3.095334395854431, Validation loss: 3.0305084444227672, Time: 33.10798621177673, LR:  1.285676464649071e-05\n",
      "Epoch 4, Training loss: 2.940088696381044, Validation loss: 3.024378118060884, Time: 32.58373427391052, LR:  1.0161240745806518e-05\n",
      "Epoch 5, Training loss: 2.8122916299210496, Validation loss: 3.024827304340544, Time: 32.72487711906433, LR:  8.03085506604659e-06\n",
      "Epoch 6, Training loss: 2.720345694990553, Validation loss: 3.011416826929365, Time: 32.692055225372314, LR:  6.3471218432121795e-06\n",
      "Epoch 7, Training loss: 2.653238995540777, Validation loss: 3.0263493685495284, Time: 32.6861777305603, LR:  5.016396804731909e-06\n",
      "Epoch 8, Training loss: 2.5855206829556345, Validation loss: 3.0421436457406905, Time: 32.64335298538208, LR:  3.964668951398179e-06\n",
      "Early stopping triggered at epoch 8\n",
      "[I 2024-08-24 21:36:47,161] Trial 23 finished with value: 3.0421436457406905 and parameters: {'lr': 2.0582664712018654e-05, 'weight_decay': 0.0002766263495154232, 'step_size': 1, 'gamma': 0.7903419736768735}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.281868741357115, Validation loss: 3.439898513612293, Time: 32.40628743171692, LR:  4.252403646522832e-06\n",
      "Epoch 2, Training loss: 3.8225569767359446, Validation loss: 3.3000052883511497, Time: 32.452725410461426, LR:  3.86798585044309e-06\n",
      "Epoch 3, Training loss: 3.67380253520943, Validation loss: 3.2398925054640997, Time: 32.492178201675415, LR:  3.5183194689106576e-06\n",
      "Epoch 4, Training loss: 3.5808227972166073, Validation loss: 3.198958090373448, Time: 32.685526609420776, LR:  3.200262969911787e-06\n",
      "Epoch 5, Training loss: 3.5118571421098426, Validation loss: 3.167828400929769, Time: 32.610668659210205, LR:  2.910958816300909e-06\n",
      "Epoch 6, Training loss: 3.440435420831985, Validation loss: 3.1483881246475947, Time: 32.760655641555786, LR:  2.6478077926307293e-06\n",
      "Epoch 7, Training loss: 3.3739383128973155, Validation loss: 3.13011368115743, Time: 35.60915422439575, LR:  2.4084456528399375e-06\n",
      "Epoch 8, Training loss: 3.3372156098044132, Validation loss: 3.1242581889742898, Time: 41.21074032783508, LR:  2.190721878992733e-06\n",
      "Epoch 9, Training loss: 3.301495511856305, Validation loss: 3.1087925207047236, Time: 40.64723324775696, LR:  1.9926803602308253e-06\n",
      "Epoch 10, Training loss: 3.2691086234425653, Validation loss: 3.1038586979820613, Time: 40.07804870605469, LR:  1.812541818350472e-06\n",
      "Epoch 11, Training loss: 3.2267541165887956, Validation loss: 3.096611295427595, Time: 41.15885519981384, LR:  1.648687822109451e-06\n",
      "Epoch 12, Training loss: 3.2038265981617764, Validation loss: 3.086585828236171, Time: 41.58549690246582, LR:  1.4996462466425813e-06\n",
      "Epoch 13, Training loss: 3.1740885937707666, Validation loss: 3.0746916702815463, Time: 41.58728814125061, LR:  1.364078047347815e-06\n",
      "Epoch 14, Training loss: 3.1723523542020446, Validation loss: 3.077225685119629, Time: 40.30604600906372, LR:  1.240765229414601e-06\n",
      "Epoch 15, Training loss: 3.1337959004577094, Validation loss: 3.066572802407401, Time: 32.84604001045227, LR:  1.1285999049083176e-06\n",
      "Epoch 16, Training loss: 3.121538248993236, Validation loss: 3.069255056835356, Time: 32.671268701553345, LR:  1.026574339095575e-06\n",
      "Epoch 17, Training loss: 3.106522217304749, Validation loss: 3.0612623805091497, Time: 32.52338743209839, LR:  9.337718965828967e-07\n",
      "Epoch 18, Training loss: 3.090447235389574, Validation loss: 3.061777943656558, Time: 32.8425076007843, LR:  8.493588059255419e-07\n",
      "Epoch 19, Training loss: 3.0797929128951576, Validation loss: 3.054180247443063, Time: 32.73339867591858, LR:  7.725766687166713e-07\n",
      "Epoch 20, Training loss: 3.0615935967518735, Validation loss: 3.0569999672117687, Time: 38.14317536354065, LR:  7.02735645855744e-07\n",
      "[I 2024-08-24 21:48:50,059] Trial 24 finished with value: 3.0569999672117687 and parameters: {'lr': 4.252403646522832e-06, 'weight_decay': 0.00014467988140267617, 'step_size': 1, 'gamma': 0.9095998809063955}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.035276333961261, Validation loss: 3.2898387000674294, Time: 40.93579864501953, LR:  8.742184022844054e-06\n",
      "Epoch 2, Training loss: 3.636104266319049, Validation loss: 3.2071632771264937, Time: 40.84844994544983, LR:  6.012403297064459e-06\n",
      "Epoch 3, Training loss: 3.516057281804508, Validation loss: 3.1641985461825417, Time: 40.78145146369934, LR:  4.135007146050833e-06\n",
      "Epoch 4, Training loss: 3.4050119198285618, Validation loss: 3.142463831674485, Time: 41.541568756103516, LR:  2.8438351941959125e-06\n",
      "Epoch 5, Training loss: 3.3503106529190694, Validation loss: 3.1263766402289983, Time: 40.92454767227173, LR:  1.9558366711581695e-06\n",
      "Epoch 6, Training loss: 3.3394400671388977, Validation loss: 3.124397107533046, Time: 40.153520345687866, LR:  1.3451191166261178e-06\n",
      "Epoch 7, Training loss: 3.280823680776111, Validation loss: 3.113968247459048, Time: 37.47210669517517, LR:  9.251004772507944e-07\n",
      "Epoch 8, Training loss: 3.2667491668780175, Validation loss: 3.1127244517916726, Time: 32.72683310508728, LR:  6.362342802444344e-07\n",
      "Epoch 9, Training loss: 3.2607911430167023, Validation loss: 3.1093649864196777, Time: 32.313400745391846, LR:  4.3756766893161377e-07\n",
      "Epoch 10, Training loss: 3.258793688384739, Validation loss: 3.108757507233393, Time: 32.51347827911377, LR:  3.009354743046655e-07\n",
      "Epoch 11, Training loss: 3.2520922641076986, Validation loss: 3.1078167302267894, Time: 32.42049312591553, LR:  2.0696721015996204e-07\n",
      "Epoch 12, Training loss: 3.2384972339551124, Validation loss: 3.1070182096390497, Time: 32.600507974624634, LR:  1.4234089942494295e-07\n",
      "Epoch 13, Training loss: 3.2397356103863237, Validation loss: 3.1061006727672757, Time: 35.54624819755554, LR:  9.78944038209837e-08\n",
      "Epoch 14, Training loss: 3.248740554561276, Validation loss: 3.1063599359421503, Time: 40.26051044464111, LR:  6.732649813358216e-08\n",
      "Epoch 15, Training loss: 3.2437810495760315, Validation loss: 3.106539181300572, Time: 40.41192150115967, LR:  4.6303539058477035e-08\n",
      "Early stopping triggered at epoch 15\n",
      "[I 2024-08-24 21:58:15,022] Trial 25 finished with value: 3.106539181300572 and parameters: {'lr': 8.742184022844054e-06, 'weight_decay': 0.0009580864211535052, 'step_size': 1, 'gamma': 0.6877461377332654}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.32237710332024, Validation loss: 3.519562255768549, Time: 41.25440692901611, LR:  2.3706082789814935e-06\n",
      "Epoch 2, Training loss: 3.990796427754961, Validation loss: 3.3923841544560025, Time: 40.72632336616516, LR:  2.3706082789814935e-06\n",
      "Epoch 3, Training loss: 3.8411634885347805, Validation loss: 3.3341239406948997, Time: 41.0351676940918, LR:  1.943739441408955e-06\n",
      "Epoch 4, Training loss: 3.749569830104444, Validation loss: 3.279992875598726, Time: 41.082558393478394, LR:  1.943739441408955e-06\n",
      "Epoch 5, Training loss: 3.6557915231885287, Validation loss: 3.2431099868956066, Time: 39.88723540306091, LR:  1.5937356878345278e-06\n",
      "Epoch 6, Training loss: 3.611859463375701, Validation loss: 3.2221048105330694, Time: 32.86946392059326, LR:  1.5937356878345278e-06\n",
      "Epoch 7, Training loss: 3.561686789495705, Validation loss: 3.202486276626587, Time: 33.000160694122314, LR:  1.3067561364275425e-06\n",
      "Epoch 8, Training loss: 3.5704390614695805, Validation loss: 3.193868841443743, Time: 33.02783465385437, LR:  1.3067561364275425e-06\n",
      "Epoch 9, Training loss: 3.488660568316307, Validation loss: 3.1816014562334334, Time: 32.60401940345764, LR:  1.071452194442127e-06\n",
      "Epoch 10, Training loss: 3.458041153010532, Validation loss: 3.1689044748033797, Time: 32.71086263656616, LR:  1.071452194442127e-06\n",
      "Epoch 11, Training loss: 3.4367765089463904, Validation loss: 3.164001669202532, Time: 38.69791078567505, LR:  8.785187786554581e-07\n",
      "Epoch 12, Training loss: 3.4359999288468672, Validation loss: 3.1571990308307467, Time: 40.53464412689209, LR:  8.785187786554581e-07\n",
      "Epoch 13, Training loss: 3.396931744891511, Validation loss: 3.150169293085734, Time: 40.70805644989014, LR:  7.203263462931525e-07\n",
      "Epoch 14, Training loss: 3.3922017149671295, Validation loss: 3.152849560692197, Time: 40.73841595649719, LR:  7.203263462931525e-07\n",
      "Epoch 15, Training loss: 3.3691493138759094, Validation loss: 3.142719280152094, Time: 40.858009338378906, LR:  5.906191851222064e-07\n",
      "Epoch 16, Training loss: 3.372035183850125, Validation loss: 3.1346691790081205, Time: 41.09020495414734, LR:  5.906191851222064e-07\n",
      "Epoch 17, Training loss: 3.359874098258611, Validation loss: 3.136072249639602, Time: 41.046921253204346, LR:  4.842680316075162e-07\n",
      "Epoch 18, Training loss: 3.3447540547015397, Validation loss: 3.127847841807774, Time: 37.5900673866272, LR:  4.842680316075162e-07\n",
      "Epoch 19, Training loss: 3.3288795016926422, Validation loss: 3.1266532852536155, Time: 32.470813035964966, LR:  3.970672344287194e-07\n",
      "Epoch 20, Training loss: 3.3261311689072106, Validation loss: 3.126036223911104, Time: 32.43659567832947, LR:  3.970672344287194e-07\n",
      "[I 2024-08-24 22:10:52,463] Trial 26 finished with value: 3.126036223911104 and parameters: {'lr': 2.3706082789814935e-06, 'weight_decay': 0.00048424854571077594, 'step_size': 2, 'gamma': 0.8199327820807502}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.8661655914148634, Validation loss: 3.2209476629892984, Time: 32.668952226638794, LR:  4.814317653849026e-05\n",
      "Epoch 2, Training loss: 3.6220351048475186, Validation loss: 3.080215578987485, Time: 32.77739429473877, LR:  4.814317653849026e-05\n",
      "Epoch 3, Training loss: 2.876067742088137, Validation loss: 3.073196229480562, Time: 32.68496084213257, LR:  3.685072229389986e-05\n",
      "Epoch 4, Training loss: 2.6368781987731977, Validation loss: 3.059189961070106, Time: 40.236976623535156, LR:  3.685072229389986e-05\n",
      "Epoch 5, Training loss: 2.3857984084349413, Validation loss: 3.0806999717439925, Time: 40.20288038253784, LR:  2.8207023948584546e-05\n",
      "Epoch 6, Training loss: 2.2205604632225264, Validation loss: 3.1468866779690696, Time: 40.34736728668213, LR:  2.8207023948584546e-05\n",
      "Early stopping triggered at epoch 6\n",
      "[I 2024-08-24 22:14:34,550] Trial 27 finished with value: 3.1468866779690696 and parameters: {'lr': 4.814317653849026e-05, 'weight_decay': 0.0002174064181905324, 'step_size': 2, 'gamma': 0.7654401920163675}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.912902654275386, Validation loss: 3.2136661552247547, Time: 40.38731646537781, LR:  1.4386667308124549e-05\n",
      "Epoch 2, Training loss: 3.4855173297182342, Validation loss: 3.12299598966326, Time: 40.74946999549866, LR:  1.272395240589535e-05\n",
      "Epoch 3, Training loss: 3.2734537872337026, Validation loss: 3.0699967429751442, Time: 40.910661697387695, LR:  1.1253402984863717e-05\n",
      "Epoch 4, Training loss: 3.107656894350898, Validation loss: 3.0349448862529935, Time: 41.37161898612976, LR:  9.952809842409056e-06\n",
      "Epoch 5, Training loss: 2.9729233542842977, Validation loss: 3.045021550995963, Time: 36.571725845336914, LR:  8.802530567188625e-06\n",
      "Epoch 6, Training loss: 2.882491519465249, Validation loss: 3.0175546350933256, Time: 32.69326376914978, LR:  7.78519288654822e-06\n",
      "Epoch 7, Training loss: 2.7889412824924174, Validation loss: 3.0233727296193442, Time: 32.769798040390015, LR:  6.885432299057444e-06\n",
      "Epoch 8, Training loss: 2.7236408778196255, Validation loss: 3.024428305171785, Time: 32.76009702682495, LR:  6.089660029723894e-06\n",
      "Early stopping triggered at epoch 8\n",
      "[I 2024-08-24 22:19:35,337] Trial 28 finished with value: 3.024428305171785 and parameters: {'lr': 1.4386667308124549e-05, 'weight_decay': 0.00011046666579712426, 'step_size': 1, 'gamma': 0.8844266801603025}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.8973826035945374, Validation loss: 3.1986766202109203, Time: 32.6709566116333, LR:  1.5060927392333327e-05\n",
      "Epoch 2, Training loss: 3.439018914685447, Validation loss: 3.1340192499614896, Time: 36.304535150527954, LR:  1.5060927392333327e-05\n",
      "Epoch 3, Training loss: 3.2158532530598385, Validation loss: 3.263379528408959, Time: 44.93193769454956, LR:  1.4237404249006781e-05\n",
      "Epoch 4, Training loss: 3.093725522594339, Validation loss: 3.0766779468173073, Time: 44.46019911766052, LR:  1.4237404249006781e-05\n",
      "Epoch 5, Training loss: 2.953065414753186, Validation loss: 3.0532740297771634, Time: 45.040313959121704, LR:  1.345891089368251e-05\n",
      "Epoch 6, Training loss: 2.7827421042340745, Validation loss: 3.03845637185233, Time: 44.73350477218628, LR:  1.345891089368251e-05\n",
      "Epoch 7, Training loss: 2.646946154755248, Validation loss: 3.046262298311506, Time: 44.9734582901001, LR:  1.272298512256702e-05\n",
      "Epoch 8, Training loss: 2.5449094564251644, Validation loss: 3.0525628668921336, Time: 45.00102233886719, LR:  1.272298512256702e-05\n",
      "Early stopping triggered at epoch 8\n",
      "[I 2024-08-24 22:25:16,967] Trial 29 finished with value: 3.0525628668921336 and parameters: {'lr': 1.5060927392333327e-05, 'weight_decay': 9.376087436926285e-05, 'step_size': 2, 'gamma': 0.9453205555093669}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.909003950435029, Validation loss: 3.2357649008433023, Time: 44.78004789352417, LR:  9.744531612442049e-05\n",
      "Epoch 2, Training loss: 3.0707559324580536, Validation loss: 3.200322049004691, Time: 44.636507987976074, LR:  9.744531612442049e-05\n",
      "Epoch 3, Training loss: 2.613048629295191, Validation loss: 3.248954352878389, Time: 45.3936653137207, LR:  9.744531612442049e-05\n",
      "Epoch 4, Training loss: 2.1715714257849745, Validation loss: 3.322023550669352, Time: 45.40885639190674, LR:  8.607554115864969e-05\n",
      "Early stopping triggered at epoch 4\n",
      "[I 2024-08-24 22:28:20,501] Trial 30 finished with value: 3.322023550669352 and parameters: {'lr': 9.744531612442049e-05, 'weight_decay': 5.965758379460889e-05, 'step_size': 3, 'gamma': 0.8833214830843834}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 4.084265343536287, Validation loss: 3.310878560656593, Time: 45.82299017906189, LR:  6.863657285207923e-06\n",
      "Epoch 2, Training loss: 3.6847213309192095, Validation loss: 3.218529848825364, Time: 45.38321375846863, LR:  5.927000634119102e-06\n",
      "Epoch 3, Training loss: 3.5254672321342153, Validation loss: 3.159661894752866, Time: 45.81678104400635, LR:  5.118165878205565e-06\n",
      "Epoch 4, Training loss: 3.4161273607840905, Validation loss: 3.1380714234851657, Time: 45.50921034812927, LR:  4.41970965989631e-06\n",
      "Epoch 5, Training loss: 3.344861088419807, Validation loss: 3.1105443295978366, Time: 45.4310998916626, LR:  3.816569048877591e-06\n",
      "Epoch 6, Training loss: 3.2854119235947286, Validation loss: 3.1008928503308977, Time: 45.015685081481934, LR:  3.2957366944307683e-06\n",
      "Epoch 7, Training loss: 3.2324375097568216, Validation loss: 3.0775063832600913, Time: 44.848766565322876, LR:  2.8459803084688853e-06\n",
      "Epoch 8, Training loss: 3.1820324229065484, Validation loss: 3.0719616753714427, Time: 44.807032108306885, LR:  2.457600429633714e-06\n",
      "Epoch 9, Training loss: 3.14832561114836, Validation loss: 3.063962391444615, Time: 44.88642120361328, LR:  2.122221244385622e-06\n",
      "Epoch 10, Training loss: 3.1117774607867177, Validation loss: 3.057383446466355, Time: 44.857444047927856, LR:  1.8326099539268546e-06\n",
      "Epoch 11, Training loss: 3.1023094202639787, Validation loss: 3.0523216610863093, Time: 35.3997061252594, LR:  1.5825207914192068e-06\n",
      "Epoch 12, Training loss: 3.1055915962309526, Validation loss: 3.0519388062613353, Time: 32.586419343948364, LR:  1.3665603255661625e-06\n",
      "Epoch 13, Training loss: 3.063107582239004, Validation loss: 3.051564034961519, Time: 32.576565980911255, LR:  1.180071145692014e-06\n",
      "Epoch 14, Training loss: 3.039320947150507, Validation loss: 3.052008583432152, Time: 32.61268734931946, LR:  1.019031419866463e-06\n",
      "Epoch 15, Training loss: 3.0307996004996216, Validation loss: 3.0471354779743014, Time: 32.596357107162476, LR:  8.799681599418393e-07\n",
      "Epoch 16, Training loss: 3.0069754039042094, Validation loss: 3.047612116450355, Time: 32.54917073249817, LR:  7.598823229738085e-07\n",
      "Epoch 17, Training loss: 2.997824481253088, Validation loss: 3.0450003885087513, Time: 32.59355187416077, LR:  6.561841337602893e-07\n",
      "Epoch 18, Training loss: 2.987682517463639, Validation loss: 3.044341200873965, Time: 32.59572720527649, LR:  5.666372336622735e-07\n",
      "Epoch 19, Training loss: 2.9788865387087036, Validation loss: 3.0421145302908763, Time: 32.60541296005249, LR:  4.893104512181438e-07\n",
      "Epoch 20, Training loss: 2.9759843278918745, Validation loss: 3.0415387664522444, Time: 32.55876708030701, LR:  4.225361544349293e-07\n",
      "[I 2024-08-24 22:41:24,544] Trial 31 finished with value: 3.0415387664522444 and parameters: {'lr': 6.863657285207923e-06, 'weight_decay': 0.00010288988703739155, 'step_size': 1, 'gamma': 0.8635338840260224}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.905490608610345, Validation loss: 3.174162819271996, Time: 32.575666666030884, LR:  2.3817419786657342e-05\n",
      "Epoch 2, Training loss: 3.3882835729587715, Validation loss: 3.1606314863477434, Time: 33.37192487716675, LR:  1.788480436044141e-05\n",
      "Epoch 3, Training loss: 3.188646025911591, Validation loss: 3.064185380935669, Time: 44.406330585479736, LR:  1.3429927753570309e-05\n",
      "Epoch 4, Training loss: 3.026158928871155, Validation loss: 3.022832064401536, Time: 44.59123659133911, LR:  1.0084704077896134e-05\n",
      "Epoch 5, Training loss: 2.922162038334728, Validation loss: 3.026543384506589, Time: 45.532021284103394, LR:  7.57273294427797e-06\n",
      "Epoch 6, Training loss: 2.831043918457257, Validation loss: 3.023229269754319, Time: 45.677340030670166, LR:  5.6864617744258544e-06\n",
      "Early stopping triggered at epoch 6\n",
      "[I 2024-08-24 22:45:33,510] Trial 32 finished with value: 3.023229269754319 and parameters: {'lr': 2.3817419786657342e-05, 'weight_decay': 0.00015230718390799928, 'step_size': 1, 'gamma': 0.750912757160227}. Best is trial 9 with value: 3.0216321774891446.\n",
      "Epoch 1, Training loss: 3.868183282000073, Validation loss: 3.1490323657081243, Time: 45.239338874816895, LR:  2.628782049369984e-05\n",
      "Epoch 2, Training loss: 3.3158716182031576, Validation loss: 3.0524561518714544, Time: 45.03022003173828, LR:  1.8954327371605432e-05\n",
      "Epoch 3, Training loss: 3.05672730143959, Validation loss: 3.0267441045670282, Time: 45.74869918823242, LR:  1.3666653201473777e-05\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, PolynomialLR,StepLR, LambdaLR\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the trails for the hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-6, 1e-4, log = True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log = True)\n",
    "    step_size =  trial.suggest_int('step_size', 1, 3)  \n",
    "    gamma = trial.suggest_float('gamma', 0.5, 0.99)     \n",
    "\n",
    "    # Define the model\n",
    "    torch.cuda.empty_cache() \n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "\n",
    "    if choosen_model == 0:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", config = config)\n",
    "    elif choosen_model== 1:\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base', config = config)\n",
    "    elif choosen_model == 2:\n",
    "        model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased', config = config)\n",
    "    else:\n",
    "        print(\"Error: wrong number has been used. Use 1,2 or 3 to select the model\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "        \n",
    "    # Define the optimizer to train the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma)\n",
    "\n",
    "    val_loss_history = [] \n",
    "    count = 0\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    for epoch in (range(EPOCHS)):\n",
    "        train_loss, val_loss = train(model,optimizer, epoch, training_loader, validation_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if len(val_loss_history) > 1 :\n",
    "            if val_loss >= val_loss_history[-2] or val_loss > min(val_loss_history):\n",
    "                count += 1\n",
    "                if count >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "            else:\n",
    "                count = 0\n",
    "    \n",
    "    # Use the last value of validation loss as score\n",
    "    return val_loss_history[-1]\n",
    "\n",
    "patience = 2\n",
    "\n",
    "# Create a study using Optune to minimize the validation loss\n",
    "study = optuna.create_study(direction = 'minimize', study_name = \"Hyperparameter Optimization BART\")\n",
    "study.optimize(objective, n_trials = 100, show_progress_bar = True, timeout = 600 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameter setting found using Optuna\n",
    "print(\"\\n\\nBest trial: \")\n",
    "print(\"\\nValue: \", study.best_trial.value)\n",
    "print(\"\\nParams: \")\n",
    "\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h4> Train the model with the found hyperparameters </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from collections import deque\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, PolynomialLR,StepLR, LambdaLR\n",
    "\n",
    "#l_rates = []\n",
    "loss_values = []\n",
    "val_losses = []\n",
    "patience = 2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # Rilascio della memoria GPU\n",
    "\n",
    "# Eliminazione del modello se esiste\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "\n",
    "if choosen_model == 0:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", config = config)\n",
    "elif choosen_model == 1:\n",
    "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-base', config = config)\n",
    "elif choosen_model == 2:\n",
    "    model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased', config = config)\n",
    "else:\n",
    "    raise TypeError(\"Error: wrong number has been used. Use 1,2 or 3 to select the model\")\n",
    "\n",
    "model = model.to(device)\n",
    "'''\n",
    "for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        if \"decoder\" in str(name):  \n",
    "            if \"Dense\" in str(name): # Freeza tutti i parametri tranne quelli della testa\n",
    "                param.requires_grad = True\n",
    "'''\n",
    "\n",
    "def get_lr_lambda(current_step, warmup_steps, gamma, step_size):\n",
    "    if current_step < warmup_steps:\n",
    "        # Linear warmup\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # After warmup, apply StepLR-like behavior\n",
    "        steps_since_warmup = current_step - warmup_steps\n",
    "        return gamma ** (steps_since_warmup // step_size)\n",
    "\n",
    "\n",
    "warmup_steps = 3  # Number of steps to warm up\n",
    "gamma = 0.6  # Decay rate for StepLR\n",
    "step_size = 1  # Step size for StepLR\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, weight_decay = 1e-4)\n",
    "\n",
    "# Create the LambdaLR with warmup and StepLR-like behavior\n",
    "scheduler = LambdaLR(optimizer, lr_lambda = lambda step: get_lr_lambda(step, warmup_steps, gamma, step_size))\n",
    "#scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma)  \n",
    "\n",
    "val_loss_history = []\n",
    "count = 0\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "  train_loss, val_loss = train(model, optimizer, epoch, training_loader, validation_loader)\n",
    "  val_loss_history.append(val_loss)\n",
    "  val_losses.append(val_loss)\n",
    "  loss_values.append(train_loss)\n",
    "  scheduler.step()\n",
    "  \n",
    "# Implement some early stopping\n",
    "  if len(val_loss_history) > 1 :\n",
    "    if val_loss >= val_loss_history[-2] or val_loss > min(val_loss_history):\n",
    "        count += 1\n",
    "        if count >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "    else:\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title(\"Loss curves\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Test the performance of the model generating a summary </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testo di input: the rey-osterrieth complex figure test (rocft) was designed to examine the visuospatial ability and memory in patients who suﬀer from traumatic brain injury. additionally, the test is utilized to test for dementia and to evaluate children’s cognitive development. the test procedure starts with presenting the ﬁgure depicted in fig. 1 to the patient, who is subsequently asked to copy it by drawing it, typically with a pen on paper. after 3 min, the patient is asked to reproduce the ﬁgure from memory. this procedure is repeated after 30 min. b. schuster et al. those three steps are called copy, immediate recall, and delayed recall. while copy is always part of the procedure, sometimes only one of the steps immediate recall or delayed recall are carried out. the ﬁgure can be subdivided into 18 separate sections, as annotated in fig. 1. according to the osterrieth scoring system, for each section, a score ranging from 0 to 2 is determined in the following way: if the unit is drawn and placed correctly, 2 points are assigned. in case it is placed poorly, this corresponds to only 1 point. in the case of a distorted section, which might be incomplete but still recognizable, 1 or 0.5 points are given depending on the placement quality. 0 points are assigned when the section is absent or unrecognizable. once all sections are scored, the sum of them represents the score of the entire drawing and can range from 0 to 36. the rocft has become one of the most widely applied neuropsychological tests for constructional and non-verbal memory and can evaluate the patient’s neuropsychological dysfunction. there are several abilities necessary for good test performance, such as working memory, visuospatial abilities, planning, problem-solving, and visuomotor coordination. consequently, the test provides valuable data for evaluating the progress of patients through treatment and represents a tool for research into the organization of brain activity and its connection to behavior, brain disorders, and behavioral disabilities. manual scoring of the rocft represents a monotone and repetitive task. moreover, the resulting score depends to some extent on the subjective judgments of the rater, thereby causing inter-rater variability. hence, an automated scoring system could set a new standard to combat inter-rater variability. given the popularity and wide acceptance of this test, automation holds great potential to reduce time and e�\n",
      "Target: the rey-osterrieth complex figure test (rocft) is a widely used neuropsychological tool for assessing the presence and severity of diﬀerent diseases. it involves presenting a complex illustration to the patient who is asked to copy it, followed by recall from memory after 3 and 30 min. in clinical practice, a human rater evaluates each component of the reproduction, with the overall score indicating illness severity. however, this method is both time-consuming and error-prone. eﬀorts have been made to automate the process, but current algorithms require large-scale private datasets of up to 20,000 illustrations. with limited data, training a deep learning model is challenging. this study addresses this challenge by developing a ﬁne-tuning strategy with multiple stages. we show that pre-training on a large-scale sketch dataset with initialized weights from imagenet signiﬁcantly reduces the mean absolute error (mae) compared to just training with initialized weights from imagenet, e.g., rexnet-200 from 3.1 to 2.2 mae. additionally, techniques such as stochastic weight averaging (swa) and ensembling of diﬀerent architectures can further reduce the error to an mae of 1.97.\n",
      "Riassunto generato:  the rey-osterrieth complex figure test (rocft) was designed to examine the visuospatial ability and memory in patients who suﬀer from traumatic brain injury. additionally, the test is utilized to test for dementia and to evaluate children’s cognitive development. the rocft is designed to evaluate the performance of the ﬁgure depicted in fig. 1 to the patient, who is subsequently asked to copy it by drawing it, typically with a pen on paper. after 3 min, the patient is asked to perform a series of three steps: copy, immediate recall, and delayed recall. in the case of a distorted section, the section is missing or unrecognizable, the sum of them represents the score of the whole task. in this way, the score is determined by a combination of the subjective judgments of the rater and the subjective judgment of the patient. the resulting score can range from 0 to 36.\n"
     ]
    }
   ],
   "source": [
    "# Select a sample of the training set\n",
    "sample = testing_set[0]  \n",
    "\n",
    "input_ids = sample['source_ids'].unsqueeze(0).to(device)\n",
    "target_ids = sample['target_ids'].unsqueeze(0).to(device)\n",
    "input_mask = sample['source_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "# Generate the summary\n",
    "with torch.no_grad():\n",
    "\n",
    "    summary_ids = model.generate(input_ids, attention_mask = input_mask,\n",
    "                                    num_beams = 5,\n",
    "                                    no_repeat_ngram_size = 3,\n",
    "                                    min_length = 100,\n",
    "                                    max_length = 250,\n",
    "                                    early_stopping = False)\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(\"Testo di input:\", tokenizer.decode(input_ids[0], skip_special_tokens = True))\n",
    "print(\"Target:\", tokenizer.decode(target_ids[0], skip_special_tokens = True))\n",
    "print(\"Riassunto generato:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Compute the ROUGE score for the trained model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Avg F1: 0.43972721828138245\n",
      "ROUGE-2 Avg F1: 0.1332763230065144\n",
      "ROUGE-L Avg F1: 0.22396936036696274\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def compute_rouge_score(model, test_set, tokenizer):\n",
    "\n",
    "    rouge_scores = []\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = True)\n",
    "    \n",
    "    for i in range(len(test_set)):\n",
    "\n",
    "        sample = test_set[i]\n",
    "        input_ids = sample['source_ids'].unsqueeze(0).to(device)\n",
    "        target_ids = sample['target_ids'].unsqueeze(0).to(device)\n",
    "        input_mask = sample['source_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate the summary\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, attention_mask=input_mask,\n",
    "                                        num_beams = 5,\n",
    "                                        no_repeat_ngram_size = 2,\n",
    "                                        min_length = 100,\n",
    "                                        max_length = 250,\n",
    "                                        early_stopping = False)\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "        # Compute the rouge score between the abstract and the generated summary\n",
    "        target_summary = tokenizer.decode(target_ids[0], skip_special_tokens = True)\n",
    "        rouge_score = rouge.score(target_summary, summary)\n",
    "\n",
    "        rouge_scores.append(rouge_score)\n",
    "\n",
    "    # Compute the average rouge score\n",
    "    rouge_1_avg = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
    "    rouge_2_avg = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
    "    rouge_L_avg = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
    "\n",
    "    print(f'ROUGE-1 Avg F1: {rouge_1_avg}')\n",
    "    print(f'ROUGE-2 Avg F1: {rouge_2_avg}')\n",
    "    print(f'ROUGE-L Avg F1: {rouge_L_avg}')\n",
    "\n",
    "compute_rouge_score(model, testing_set, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Compute the n-gram novelty of the trained model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Novelty Score for n=1 (Target): 0.6841917098603981\n",
      "Average Novelty Score for n=2 (Target): 0.9176483085623549\n",
      "Average Novelty Score for n=3 (Target): 0.968106989180785\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def compute_ngram_novelty(reference, generated, n):\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "\n",
    "    weights = (0,0,0,0)\n",
    "    weights1 = tuple(weights[:n-1] + (1,) + weights[n:])\n",
    "\n",
    "    # Compute the blue score\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens, weights = weights1,  smoothing_function = SmoothingFunction().method1)\n",
    "\n",
    "    # Compute the n-gram novelty\n",
    "    novelty = 1 - bleu_score\n",
    "\n",
    "    return novelty\n",
    "\n",
    "def compute_novelty(model, test_set, tokenizer):\n",
    "    novelty_scores_input = [[] for _ in range(3)] \n",
    "    novelty_scores_target = [[] for _ in range(3)]  \n",
    "\n",
    "    for i in range(len(test_set)):\n",
    "        sample = test_set[i]\n",
    "        input_ids = sample['source_ids'].unsqueeze(0).to(device)\n",
    "        input_mask = sample['source_mask'].unsqueeze(0).to(device)\n",
    "        target_summary = tokenizer.decode(sample['target_ids'], skip_special_tokens = True)\n",
    "        input_text = tokenizer.decode(sample['source_ids'], skip_special_tokens = True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, attention_mask = input_mask,\n",
    "                                        num_beams = 5,\n",
    "                                        no_repeat_ngram_size = 2,\n",
    "                                        min_length = 100,\n",
    "                                        max_length = 250,\n",
    "                                        early_stopping = False)\n",
    "\n",
    "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "        for i in range(3):  \n",
    "            #novelty_score_input = compute_ngram_novelty(input_text, generated_summary, n = i+1)\n",
    "            #novelty_scores_input[i].append(novelty_score_input)\n",
    "\n",
    "            novelty_score_target = compute_ngram_novelty(target_summary, generated_summary, n = i+1)\n",
    "            novelty_scores_target[i].append(novelty_score_target)\n",
    "\n",
    "\n",
    "    for i in range(3): \n",
    "\n",
    "        #novelty_avg_input = sum(novelty_scores_input[i]) / len(novelty_scores_input[i])\n",
    "        novelty_avg_target = sum(novelty_scores_target[i]) / len(novelty_scores_target[i])\n",
    "\n",
    "        #print(f'Average Novelty Score for n={i+1} (Input): {novelty_avg_input}')\n",
    "        print(f'Average Novelty Score for n={i+1} (Target): {novelty_avg_target}')\n",
    "\n",
    "\n",
    "compute_novelty(model, testing_set, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Save the model if it was good </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"pytorch_models\"\n",
    "model_name = \"BART\"  \n",
    "model_path = f\"{model_dir}/{model_name}.pt\"\n",
    "\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Eventually load some fine-tuned model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"pytorch_models\"\n",
    "model_name = \"BART\"\n",
    "model_path = f\"{model_dir}/{model_name}.pt\"\n",
    "\n",
    "model = torch.load(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
